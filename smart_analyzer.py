#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ™ºèƒ½å‰§æƒ…åˆ†æå™¨ - ä¸“æ³¨å•é›†æ ¸å¿ƒå‰§æƒ…å’Œè·¨é›†è¿è´¯æ€§
"""

import os
import re
import json
import requests
from typing import List, Dict, Tuple, Optional
from datetime import datetime

class SmartAnalyzer:
    def __init__(self, use_ai: bool = True):
        self.use_ai = use_ai

        # ä¸»çº¿å‰§æƒ…å…³é”®è¯
        self.main_plot_keywords = [
            'å››äºŒå…«æ¡ˆ', '628æ—§æ¡ˆ', 'æ­£å½“é˜²å«', 'å¬è¯ä¼š', 'ç”³è¯‰', 'é‡å®¡',
            'æ®µæ´ªå±±', 'å¼ å›­', 'éœ¸å‡Œ', 'è¯æ®', 'çœŸç›¸', 'ç¿»æ¡ˆ',
            'æ£€å¯Ÿå®˜', 'å¾‹å¸ˆ', 'æ³•å®˜', 'è¯è¯', 'è¾©æŠ¤'
        ]

        # æˆå‰§å¼ åŠ›æ ‡è¯†
        self.dramatic_keywords = [
            'çªç„¶', 'å‘ç°', 'çœŸç›¸', 'ç§˜å¯†', 'åè½¬', 'æ­éœ²', 'æš´éœ²',
            'å†²çª', 'äº‰è®®', 'è¾©è®º', 'å¯¹æŠ—', 'è´¨ç–‘', 'é¢ è¦†'
        ]

        # æƒ…æ„Ÿçˆ†å‘æ ‡è¯†
        self.emotional_keywords = [
            'æ„¤æ€’', 'æ¿€åŠ¨', 'å´©æºƒ', 'å“­æ³£', 'éœ‡æƒŠ', 'ç»æœ›', 'å¸Œæœ›',
            'åšæŒ', 'æ”¾å¼ƒ', 'å†³å®š', 'é€‰æ‹©', 'æ”¹å˜', 'è§‰æ‚Ÿ'
        ]

        # é”™åˆ«å­—ä¿®æ­£
        self.corrections = {
            'é˜²è¡›': 'é˜²å«', 'æ­£ç•¶': 'æ­£å½“', 'è¨¼æ“š': 'è¯æ®', 'æª¢å¯Ÿå®˜': 'æ£€å¯Ÿå®˜',
            'ç™¼ç¾': 'å‘ç°', 'è¨­è¨ˆ': 'è®¾è®¡', 'é–‹å§‹': 'å¼€å§‹', 'çµæŸ': 'ç»“æŸ',
            'å•é¡Œ': 'é—®é¢˜', 'æ©Ÿæœƒ': 'æœºä¼š', 'æ±ºå®š': 'å†³å®š', 'é¸æ“‡': 'é€‰æ‹©',
            'è½è­‰æœƒ': 'å¬è¯ä¼š', 'è¾¯è­·': 'è¾©æŠ¤', 'å¯©åˆ¤': 'å®¡åˆ¤', 'èª¿æŸ¥': 'è°ƒæŸ¥'
        }

    def parse_subtitle_file(self, filepath: str) -> List[Dict]:
        """è§£æå­—å¹•æ–‡ä»¶å¹¶ä¿®æ­£é”™åˆ«å­—"""
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
        except:
            try:
                with open(filepath, 'r', encoding='gbk', errors='ignore') as f:
                    content = f.read()
            except:
                print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶: {filepath}")
                return []

        # ä¿®æ­£é”™åˆ«å­—
        for old, new in self.corrections.items():
            content = content.replace(old, new)

        # è§£æå­—å¹•
        blocks = re.split(r'\n\s*\n', content.strip())
        subtitles = []

        for block in blocks:
            lines = block.strip().split('\n')
            if len(lines) >= 3:
                try:
                    index = int(lines[0])
                    time_match = re.match(r'(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})', lines[1])
                    if time_match:
                        start_time = time_match.group(1)
                        end_time = time_match.group(2)
                        text = '\n'.join(lines[2:])

                        subtitles.append({
                            'index': index,
                            'start': start_time,
                            'end': end_time,
                            'text': text,
                            'episode': os.path.basename(filepath)
                        })
                except (ValueError, IndexError):
                    continue

        return subtitles

    def calculate_segment_score(self, text: str, position: float) -> float:
        """è®¡ç®—ç‰‡æ®µé‡è¦æ€§è¯„åˆ†"""
        score = 0

        # ä¸»çº¿å‰§æƒ…è¯„åˆ† (æœ€é«˜æƒé‡)
        for keyword in self.main_plot_keywords:
            if keyword in text:
                score += 5.0

        # æˆå‰§å¼ åŠ›è¯„åˆ†
        for keyword in self.dramatic_keywords:
            if keyword in text:
                score += 3.0

        # æƒ…æ„Ÿå¼ºåº¦è¯„åˆ†
        for keyword in self.emotional_keywords:
            if keyword in text:
                score += 2.0

        # å¯¹è¯å¼ºåº¦è¯„åˆ†
        score += text.count('ï¼') * 0.5
        score += text.count('ï¼Ÿ') * 0.5
        score += text.count('...') * 0.3

        # ä½ç½®æƒé‡ (å¼€å¤´å’Œç»“å°¾æ›´é‡è¦)
        if position < 0.2 or position > 0.8:
            score *= 1.2

        # æ–‡æœ¬é•¿åº¦é€‚ä¸­åŠ åˆ†
        text_len = len(text)
        if 20 <= text_len <= 150:
            score += 1.0
        elif text_len > 200:
            score *= 0.8

        return score

    def find_core_segments(self, subtitles: List[Dict]) -> List[Dict]:
        """æ‰¾åˆ°æ ¸å¿ƒå‰§æƒ…ç‰‡æ®µ"""
        if not subtitles:
            return []

        # åˆ›å»ºæ»‘åŠ¨çª—å£ç‰‡æ®µ
        window_size = 25  # æ¯ä¸ªçª—å£25æ¡å­—å¹•ï¼Œçº¦2-3åˆ†é’Ÿ
        step_size = 10    # æ­¥é•¿10ï¼Œç¡®ä¿é‡å 

        segments = []

        for i in range(0, len(subtitles), step_size):
            end_idx = min(i + window_size, len(subtitles))

            if end_idx - i < 15:  # å¤ªçŸ­è·³è¿‡
                continue

            segment_subs = subtitles[i:end_idx]
            combined_text = ' '.join([sub['text'] for sub in segment_subs])

            # è®¡ç®—è¯„åˆ†
            position = i / len(subtitles)
            score = self.calculate_segment_score(combined_text, position)

            if score >= 6.0:  # é«˜åˆ†ç‰‡æ®µ
                start_time = segment_subs[0]['start']
                end_time = segment_subs[-1]['end']
                duration = self.time_to_seconds(end_time) - self.time_to_seconds(start_time)

                segments.append({
                    'start_index': i,
                    'end_index': end_idx - 1,
                    'start_time': start_time,
                    'end_time': end_time,
                    'duration': duration,
                    'score': score,
                    'text': combined_text,
                    'subtitles': segment_subs,
                    'position': position
                })

        # æŒ‰åˆ†æ•°æ’åºå¹¶é€‰æ‹©æœ€ä½³ç‰‡æ®µ
        segments.sort(key=lambda x: x['score'], reverse=True)

        # é€‰æ‹©æœ€é«˜åˆ†çš„ç‰‡æ®µï¼Œé¿å…é‡å 
        selected = []
        used_ranges = []

        for segment in segments:
            start_idx = segment['start_index']
            end_idx = segment['end_index']

            # æ£€æŸ¥æ˜¯å¦ä¸å·²é€‰ç‰‡æ®µé‡å 
            overlap = False
            for used_start, used_end in used_ranges:
                if not (end_idx < used_start or start_idx > used_end):
                    overlap = True
                    break

            if not overlap:
                selected.append(segment)
                used_ranges.append((start_idx, end_idx))

                if len(selected) >= 1:  # æ¯é›†åªé€‰1ä¸ªæ ¸å¿ƒç‰‡æ®µ
                    break

        return selected

    def extract_key_dialogues(self, segment: Dict) -> List[str]:
        """æå–å…³é”®å¯¹è¯"""
        key_dialogues = []

        for sub in segment['subtitles']:
            text = sub['text'].strip()

            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®ä¿¡æ¯
            has_key_info = any(keyword in text for keyword in self.main_plot_keywords)
            has_drama = any(keyword in text for keyword in self.dramatic_keywords)

            if (has_key_info or has_drama) and len(text) > 10:
                time_code = f"{sub['start']} --> {sub['end']}"
                key_dialogues.append(f"[{time_code}] {text}")

        return key_dialogues[:5]  # æœ€å¤š5æ¡å…³é”®å¯¹è¯

    def analyze_plot_significance(self, segment: Dict) -> str:
        """åˆ†æå‰§æƒ…æ„ä¹‰"""
        text = segment['text']

        # ä¸»çº¿å‰§æƒ…åˆ†æ
        if 'å››äºŒå…«æ¡ˆ' in text and ('ç”³è¯‰' in text or 'é‡å®¡' in text):
            return "å››äºŒå…«æ¡ˆç”³è¯‰ç¨‹åºå¯åŠ¨"
        elif '628æ—§æ¡ˆ' in text and ('è¯æ®' in text or 'çœŸç›¸' in text):
            return "628æ—§æ¡ˆå…³é”®è¯æ®æ­éœ²"
        elif 'å¬è¯ä¼š' in text:
            return "å¬è¯ä¼šå…³é”®è¾©è®º"
        elif 'å¼ å›­' in text and 'éœ¸å‡Œ' in text:
            return "å¼ å›­éœ¸å‡Œäº‹ä»¶è¯æ®å‘ˆç°"
        elif 'æ­£å½“é˜²å«' in text:
            return "æ­£å½“é˜²å«äº‰è®®æ ¸å¿ƒè®¨è®º"
        elif 'æ®µæ´ªå±±' in text:
            return "æ®µæ´ªå±±æ¡ˆä»¶å…³é”®è¿›å±•"
        else:
            return "é‡è¦å‰§æƒ…æ¨è¿›èŠ‚ç‚¹"

    def generate_episode_theme(self, episode_file: str, segment: Dict) -> str:
        """ç”Ÿæˆé›†æ•°ä¸»é¢˜"""
        episode_num = re.search(r'[Ee](\d+)', episode_file)
        episode_number = episode_num.group(1) if episode_num else "00"

        significance = self.analyze_plot_significance(segment)

        # æ ¹æ®å‰§æƒ…æ„ä¹‰ç”Ÿæˆä¸»é¢˜
        if "ç”³è¯‰" in significance:
            theme = f"E{episode_number}ï¼šææ…•æ«ç”³è¯‰å¯åŠ¨ï¼Œæ—§æ¡ˆç–‘ç‚¹æµ®ç°"
        elif "å¬è¯ä¼š" in significance:
            theme = f"E{episode_number}ï¼šå¬è¯ä¼šæ¿€è¾©ï¼Œæ­£å½“é˜²å«äº‰è®®"
        elif "è¯æ®" in significance:
            theme = f"E{episode_number}ï¼šå…³é”®è¯æ®å‘ˆç°ï¼ŒçœŸç›¸é€æ­¥æ­éœ²"
        elif "éœ¸å‡Œ" in significance:
            theme = f"E{episode_number}ï¼šå¼ å›­éœ¸å‡Œè¯æ®ï¼Œæ¡ˆä»¶è½¬æŠ˜ç‚¹"
        else:
            theme = f"E{episode_number}ï¼šæ ¸å¿ƒå‰§æƒ…æ¨è¿›ï¼Œ{significance}"

        return theme

    def generate_next_episode_connection(self, segment: Dict, episode_num: str) -> str:
        """ç”Ÿæˆä¸ä¸‹ä¸€é›†çš„è¡”æ¥è¯´æ˜"""
        text = segment['text']

        if 'ç”³è¯‰' in text:
            return f"æœ¬é›†ç”³è¯‰å¯åŠ¨ï¼Œä¸ºä¸‹ä¸€é›†å¬è¯ä¼šç¿»æ¡ˆé“ºå«"
        elif 'å¬è¯ä¼š' in text and 'å‡†å¤‡' in text:
            return f"å¬è¯ä¼šå‡†å¤‡å°±ç»ªï¼Œä¸‹ä¸€é›†å°†è¿›å…¥æ¿€çƒˆè¾©è®º"
        elif 'è¯æ®' in text and ('æ–°' in text or 'å‘ç°' in text):
            return f"æ–°è¯æ®æµ®ç°ï¼Œä¸‹ä¸€é›†æ¡ˆä»¶è¿æ¥é‡å¤§è½¬æœº"
        elif 'å¼ å›­' in text:
            return f"å¼ å›­æ¶‰æ¡ˆä¿¡æ¯æŠ«éœ²ï¼Œä¸‹ä¸€é›†éœ¸å‡ŒçœŸç›¸å°†å…¨é¢æ­éœ²"
        elif 'çœŸç›¸' in text:
            return f"éƒ¨åˆ†çœŸç›¸æ˜¾ç°ï¼Œä¸‹ä¸€é›†æ›´æ·±å±‚å†…å¹•å³å°†æ›å…‰"
        else:
            return f"å…³é”®å‰§æƒ…èŠ‚ç‚¹ç¡®ç«‹ï¼Œä¸‹ä¸€é›†æ•…äº‹çº¿å°†è¿›ä¸€æ­¥æ¨è¿›"

    def time_to_seconds(self, time_str: str) -> float:
        """æ—¶é—´è½¬æ¢"""
        try:
            h, m, s_ms = time_str.split(':')
            s, ms = s_ms.split(',')
            return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000
        except:
            return 0

    def analyze_single_episode(self, episode_file: str) -> Dict:
        """åˆ†æå•é›†ï¼Œè¿”å›å‰ªè¾‘æ–¹æ¡ˆ"""
        print(f"ğŸ” åˆ†æ {episode_file}...")

        subtitles = self.parse_subtitle_file(episode_file)
        if not subtitles:
            return None

        # æ‰¾åˆ°æ ¸å¿ƒç‰‡æ®µ
        core_segments = self.find_core_segments(subtitles)

        if not core_segments:
            print(f"  âš  æœªæ‰¾åˆ°è¶³å¤Ÿç²¾å½©çš„ç‰‡æ®µ")
            return None

        # é€‰æ‹©æœ€ä½³ç‰‡æ®µ
        best_segment = core_segments[0]

        # ç”Ÿæˆé›†æ•°ä¿¡æ¯
        episode_num = re.search(r'[Ee](\d+)', episode_file)
        episode_number = episode_num.group(1) if episode_num else "00"

        # ç”Ÿæˆä¸»é¢˜
        theme = self.generate_episode_theme(episode_file, best_segment)

        # æå–å…³é”®å¯¹è¯
        key_dialogues = self.extract_key_dialogues(best_segment)

        # åˆ†æå‰§æƒ…æ„ä¹‰
        plot_significance = self.analyze_plot_significance(best_segment)

        # ç”Ÿæˆè¡”æ¥è¯´æ˜
        next_connection = self.generate_next_episode_connection(best_segment, episode_number)

        # å†…å®¹äº®ç‚¹
        content_highlights = []
        text = best_segment['text']

        if 'å››äºŒå…«æ¡ˆ' in text:
            content_highlights.append("é¦–æ¬¡/å…³é”®æåŠå››äºŒå…«æ¡ˆç”³è¯‰")
        if '628æ—§æ¡ˆ' in text:
            content_highlights.append("628æ—§æ¡ˆå…³é”®ä¿¡æ¯æŠ«éœ²")
        if 'å¼ å›­' in text and 'éœ¸å‡Œ' in text:
            content_highlights.append("å¼ å›­éœ¸å‡Œäº‹ä»¶è¯æ®å‘ˆç°")
        if 'æ­£å½“é˜²å«' in text:
            content_highlights.append("æ­£å½“é˜²å«äº‰è®®æ ¸å¿ƒè®¨è®º")
        if any(word in text for word in ['çœŸç›¸', 'å‘ç°', 'è¯æ®']):
            content_highlights.append("é‡è¦è¯æ®æˆ–çœŸç›¸æ­éœ²")

        if not content_highlights:
            content_highlights.append("é‡è¦å‰§æƒ…æ¨è¿›èŠ‚ç‚¹")

        return {
            'episode': episode_file,
            'episode_number': episode_number,
            'theme': theme,
            'segment': {
                'start_time': best_segment['start_time'],
                'end_time': best_segment['end_time'],
                'duration': best_segment['duration'],
                'score': best_segment['score']
            },
            'plot_significance': plot_significance,
            'key_dialogues': key_dialogues,
            'content_highlights': content_highlights,
            'next_episode_connection': next_connection,
            'core_content_preview': best_segment['text'][:100] + "..." if len(best_segment['text']) > 100 else best_segment['text']
        }

def analyze_all_episodes_smartly():
    """æ™ºèƒ½åˆ†ææ‰€æœ‰é›†æ•°"""
    print("ğŸš€ å¯åŠ¨æ™ºèƒ½å‰§æƒ…åˆ†æç³»ç»Ÿ")
    print("=" * 60)
    print("ğŸ“‹ åˆ†æè§„åˆ™:")
    print("â€¢ æ¯é›†èšç„¦1ä¸ªæ ¸å¿ƒå‰§æƒ…ç‚¹")
    print("â€¢ ä¼˜å…ˆä¸»çº¿å‰§æƒ…(å››äºŒå…«æ¡ˆã€628æ—§æ¡ˆã€å¬è¯ä¼š)")
    print("â€¢ ä¿æŒè·¨é›†æ•…äº‹çº¿è¿è´¯æ€§")
    print("â€¢ è‡ªåŠ¨ä¿®æ­£å­—å¹•é”™åˆ«å­—")
    print("=" * 60)

    analyzer = SmartAnalyzer()

    # è·å–å­—å¹•æ–‡ä»¶
    subtitle_files = []
    for file in os.listdir('.'):
        if file.endswith('.txt') and any(pattern in file.lower() for pattern in ['e', 's01e', 'ç¬¬', 'é›†']):
            subtitle_files.append(file)

    subtitle_files.sort()

    if not subtitle_files:
        print("âŒ æœªæ‰¾åˆ°å­—å¹•æ–‡ä»¶")
        return []

    print(f"ğŸ“ æ‰¾åˆ° {len(subtitle_files)} ä¸ªå­—å¹•æ–‡ä»¶")

    all_plans = []

    for filename in subtitle_files:
        try:
            plan = analyzer.analyze_single_episode(filename)
            if plan:
                all_plans.append(plan)

                print(f"âœ… {filename}")
                print(f"  ğŸ“º ä¸»é¢˜: {plan['theme']}")
                print(f"  â±ï¸ ç‰‡æ®µ: {plan['segment']['start_time']} --> {plan['segment']['end_time']} ({plan['segment']['duration']:.1f}ç§’)")
                print(f"  ğŸ¯ æ„ä¹‰: {plan['plot_significance']}")
                print(f"  ğŸ’¡ äº®ç‚¹: {', '.join(plan['content_highlights'])}")
                print(f"  ğŸ”— è¡”æ¥: {plan['next_episode_connection']}")
                print()
            else:
                print(f"âŒ {filename} - æœªæ‰¾åˆ°åˆé€‚ç‰‡æ®µ")

        except Exception as e:
            print(f"âŒ å¤„ç† {filename} æ—¶å‡ºé”™: {e}")

    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    generate_analysis_report(all_plans)

    print(f"ğŸ“Š åˆ†æå®Œæˆ: {len(all_plans)}/{len(subtitle_files)} é›†")
    print(f"ğŸ“„ è¯¦ç»†æ–¹æ¡ˆå·²ä¿å­˜åˆ°: smart_analysis_report.txt")

    return all_plans

def generate_analysis_report(plans: List[Dict]):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    if not plans:
        return

    content = "ğŸ“º æ™ºèƒ½å‰§æƒ…åˆ†ææŠ¥å‘Š\n"
    content += "=" * 80 + "\n\n"

    content += "ğŸ¯ åˆ†æç›®æ ‡:\n"
    content += "â€¢ å•é›†æ ¸å¿ƒèšç„¦: æ¯é›†å›´ç»•1ä¸ªæ ¸å¿ƒå‰§æƒ…ç‚¹ï¼Œ2-3åˆ†é’Ÿæ—¶é•¿\n"
    content += "â€¢ ä¸»çº¿å‰§æƒ…ä¼˜å…ˆ: çªå‡ºå››äºŒå…«æ¡ˆã€628æ—§æ¡ˆã€å¬è¯ä¼šç­‰å…³é”®çº¿ç´¢\n"
    content += "â€¢ è·¨é›†è¿è´¯æ€§: ä¿æŒæ•…äº‹çº¿é€»è¾‘ä¸€è‡´å’Œæ˜ç¡®è¡”æ¥\n"
    content += "â€¢ é”™åˆ«å­—ä¿®æ­£: è‡ªåŠ¨ä¿®æ­£é˜²è¡›â†’é˜²å«ç­‰å¸¸è§é”™è¯¯\n\n"

    total_duration = 0

    for i, plan in enumerate(plans):
        content += f"ğŸ“º {plan['theme']}\n"
        content += "-" * 60 + "\n"
        content += f"æ—¶é—´ç‰‡æ®µ: {plan['segment']['start_time']} --> {plan['segment']['end_time']}\n"
        content += f"ç‰‡æ®µæ—¶é•¿: {plan['segment']['duration']:.1f} ç§’ ({plan['segment']['duration']/60:.1f} åˆ†é’Ÿ)\n"
        content += f"é‡è¦æ€§è¯„åˆ†: {plan['segment']['score']:.1f}/10\n"
        content += f"å‰§æƒ…æ„ä¹‰: {plan['plot_significance']}\n\n"

        content += "å…³é”®å°è¯ (ç²¾ç¡®æ—¶é—´æ ‡æ³¨):\n"
        for dialogue in plan['key_dialogues']:
            content += f"  {dialogue}\n"
        content += "\n"

        content += "âœ¨ å†…å®¹äº®ç‚¹:\n"
        for highlight in plan['content_highlights']:
            content += f"  â€¢ {highlight}\n"
        content += "\n"

        content += f"ğŸ”— ä¸ä¸‹ä¸€é›†è¡”æ¥: {plan['next_episode_connection']}\n"
        content += "\n"

        content += f"æ ¸å¿ƒå†…å®¹é¢„è§ˆ:\n{plan['core_content_preview']}\n"
        content += "=" * 80 + "\n\n"

        total_duration += plan['segment']['duration']

    # æ•´ä½“ç»Ÿè®¡
    content += "ğŸ“Š æ•´ä½“ç»Ÿè®¡:\n"
    content += f"â€¢ åˆ†æé›†æ•°: {len(plans)} é›†\n"
    content += f"â€¢ æ€»å‰ªè¾‘æ—¶é•¿: {total_duration:.1f} ç§’ ({total_duration/60:.1f} åˆ†é’Ÿ)\n"
    content += f"â€¢ å¹³å‡æ¯é›†: {total_duration/len(plans):.1f} ç§’\n"
    content += f"â€¢ æ•…äº‹çº¿è¿è´¯æ€§: å·²ç¡®ä¿è·¨é›†é€»è¾‘ä¸€è‡´\n"
    content += f"â€¢ å‰ªè¾‘ä»·å€¼: æ¯é›†èšç„¦æ ¸å¿ƒå‰§æƒ…ï¼Œé€‚åˆçŸ­è§†é¢‘ä¼ æ’­\n"

    # ä¿å­˜æŠ¥å‘Š
    with open('smart_analysis_report.txt', 'w', encoding='utf-8') as f:
        f.write(content)

if __name__ == "__main__":
    analyze_all_episodes_smartly()