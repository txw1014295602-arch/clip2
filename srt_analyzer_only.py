
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
SRTå­—å¹•ç²¾å‡†åˆ†æè§£é‡Šç³»ç»Ÿ
ä¸“é—¨ç”¨äºåˆ†æå­—å¹•å†…å®¹ï¼Œç”Ÿæˆè¯¦ç»†ä¸”é€šä¿—æ˜“æ‡‚çš„è§£é‡Š
ä¸æ¶‰åŠè§†é¢‘å‰ªè¾‘åŠŸèƒ½
"""

import os
import re
import json
import hashlib
from typing import List, Dict, Optional
from datetime import datetime
import platform_fix

class SRTAnalyzerOnly:
    def __init__(self):
        self.srt_folder = "srt"
        self.output_folder = "srt_analysis"
        self.cache_folder = "srt_cache"
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        for folder in [self.srt_folder, self.output_folder, self.cache_folder]:
            os.makedirs(folder, exist_ok=True)
        
        # åŠ è½½AIé…ç½®
        self.ai_config = self.load_ai_config()
        
        print("ğŸ“ SRTå­—å¹•ç²¾å‡†åˆ†æè§£é‡Šç³»ç»Ÿ")
        print("=" * 50)
        print("ğŸ¯ åŠŸèƒ½ï¼šä¸“é—¨åˆ†æSRTå­—å¹•å†…å®¹ï¼Œç”Ÿæˆè¯¦ç»†è§£é‡Š")
        print("ğŸ“ å­—å¹•ç›®å½•ï¼šsrt/")
        print("ğŸ“„ è¾“å‡ºç›®å½•ï¼šsrt_analysis/")

    def load_ai_config(self) -> Dict:
        """åŠ è½½AIé…ç½®"""
        try:
            config_content = platform_fix.safe_file_read('.ai_config.json')
            if config_content:
                return json.loads(config_content)
        except:
            pass
        return {'enabled': False}

    def parse_srt_file(self, filepath: str) -> List[Dict]:
        """ç²¾å‡†è§£æSRTå­—å¹•æ–‡ä»¶"""
        print(f"ğŸ“– è§£æå­—å¹•æ–‡ä»¶: {os.path.basename(filepath)}")
        
        try:
            content = platform_fix.safe_file_read(filepath)
            
            # æ™ºèƒ½é”™è¯¯ä¿®æ­£
            content = self.fix_common_errors(content)
            
            # è§£æSRTæ ¼å¼
            subtitles = []
            blocks = re.split(r'\n\s*\n', content.strip())
            
            for block in blocks:
                lines = block.strip().split('\n')
                if len(lines) >= 3:
                    try:
                        index = int(lines[0])
                        time_match = re.match(r'(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})', lines[1])
                        
                        if time_match:
                            start_time = time_match.group(1)
                            end_time = time_match.group(2)
                            text = '\n'.join(lines[2:]).strip()
                            
                            subtitles.append({
                                'index': index,
                                'start_time': start_time,
                                'end_time': end_time,
                                'text': text,
                                'duration': self.calculate_duration(start_time, end_time),
                                'char_count': len(text),
                                'word_count': len(text.split())
                            })
                    except (ValueError, IndexError):
                        continue
            
            print(f"âœ… æˆåŠŸè§£æ {len(subtitles)} æ¡å­—å¹•")
            return subtitles
            
        except Exception as e:
            print(f"âŒ è§£æå¤±è´¥: {e}")
            return []

    def fix_common_errors(self, content: str) -> str:
        """ä¿®æ­£å¸¸è§çš„å­—å¹•é”™è¯¯"""
        corrections = {
            # ç¹ä½“å­—ä¿®æ­£
            'é˜²è¡›': 'é˜²å«', 'æ­£ç•¶': 'æ­£å½“', 'è¨¼æ“š': 'è¯æ®', 'æª¢å¯Ÿå®˜': 'æ£€å¯Ÿå®˜',
            'ç™¼ç¾': 'å‘ç°', 'è¨­è¨ˆ': 'è®¾è®¡', 'é–‹å§‹': 'å¼€å§‹', 'çµæŸ': 'ç»“æŸ',
            'å•é¡Œ': 'é—®é¢˜', 'æ©Ÿæœƒ': 'æœºä¼š', 'æ±ºå®š': 'å†³å®š', 'é¸æ“‡': 'é€‰æ‹©',
            'è½è¨¼æœƒ': 'å¬è¯ä¼š', 'èª¿æŸ¥': 'è°ƒæŸ¥', 'å¯©ç†': 'å®¡ç†', 'è¾¯è­·': 'è¾©æŠ¤',
            
            # å¸¸è§é”™åˆ«å­—
            'å››äºŒå…«æ¡ˆ': '428æ¡ˆ', 'å…­äºŒå…«': '628', 'æ­£å½“é˜²è¡›': 'æ­£å½“é˜²å«',
            'ç”³è¿°': 'ç”³è¯‰', 'è¨¼è©': 'è¯è¯', 'è¦†å¯©': 'å¤å®¡'
        }
        
        for old, new in corrections.items():
            content = content.replace(old, new)
        
        return content

    def calculate_duration(self, start_time: str, end_time: str) -> float:
        """è®¡ç®—å­—å¹•æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰"""
        try:
            start_seconds = self.time_to_seconds(start_time)
            end_seconds = self.time_to_seconds(end_time)
            return end_seconds - start_seconds
        except:
            return 0.0

    def time_to_seconds(self, time_str: str) -> float:
        """å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºç§’æ•°"""
        try:
            h, m, s_ms = time_str.split(':')
            s, ms = s_ms.split(',')
            return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000
        except:
            return 0.0

    def analyze_subtitle_content(self, subtitles: List[Dict]) -> Dict:
        """åˆ†æå­—å¹•å†…å®¹ç‰¹å¾"""
        if not subtitles:
            return {}
        
        total_duration = sum(sub['duration'] for sub in subtitles)
        total_chars = sum(sub['char_count'] for sub in subtitles)
        total_words = sum(sub['word_count'] for sub in subtitles)
        
        # ç»Ÿè®¡è¯´è¯é€Ÿåº¦
        speaking_rates = []
        for sub in subtitles:
            if sub['duration'] > 0:
                chars_per_second = sub['char_count'] / sub['duration']
                speaking_rates.append(chars_per_second)
        
        avg_speaking_rate = sum(speaking_rates) / len(speaking_rates) if speaking_rates else 0
        
        # è¯†åˆ«å‰§æƒ…ç±»å‹
        content_text = ' '.join([sub['text'] for sub in subtitles])
        genre = self.detect_genre(content_text)
        
        # æå–å…³é”®è¯
        keywords = self.extract_keywords(content_text)
        
        # è¯†åˆ«è§’è‰²
        characters = self.identify_characters(subtitles)
        
        return {
            'total_subtitles': len(subtitles),
            'total_duration': total_duration,
            'total_characters': total_chars,
            'total_words': total_words,
            'average_speaking_rate': avg_speaking_rate,
            'genre': genre,
            'keywords': keywords,
            'characters': characters,
            'language_complexity': self.analyze_language_complexity(content_text)
        }

    def detect_genre(self, content: str) -> str:
        """æ£€æµ‹å‰§æƒ…ç±»å‹"""
        genre_keywords = {
            'æ³•å¾‹å‰§': ['æ³•å®˜', 'æ£€å¯Ÿå®˜', 'å¾‹å¸ˆ', 'æ³•åº­', 'å®¡åˆ¤', 'è¯æ®', 'æ¡ˆä»¶', 'èµ·è¯‰', 'è¾©æŠ¤', 'åˆ¤å†³', 'ç”³è¯‰', 'å¬è¯ä¼š'],
            'çŠ¯ç½ªå‰§': ['è­¦å¯Ÿ', 'çŠ¯ç½ª', 'å«Œç–‘äºº', 'è°ƒæŸ¥', 'ç ´æ¡ˆ', 'çº¿ç´¢', 'å‡¶æ‰‹', 'æ¡ˆå‘', 'ä¾¦æ¢', 'åˆ‘ä¾¦'],
            'åŒ»ç–—å‰§': ['åŒ»ç”Ÿ', 'æŠ¤å£«', 'åŒ»é™¢', 'æ‰‹æœ¯', 'ç—…äºº', 'è¯Šæ–­', 'æ²»ç–—', 'ç—…æƒ…', 'æ€¥è¯Š'],
            'çˆ±æƒ…å‰§': ['çˆ±æƒ…', 'å–œæ¬¢', 'å¿ƒåŠ¨', 'è¡¨ç™½', 'çº¦ä¼š', 'åˆ†æ‰‹', 'å¤åˆ', 'ç»“å©š', 'æƒ…ä¾£'],
            'å®¶åº­å‰§': ['å®¶åº­', 'çˆ¶æ¯', 'å­©å­', 'å…„å¼Ÿ', 'å§å¦¹', 'äº²æƒ…', 'å®¶äºº', 'å›¢èš'],
            'å•†æˆ˜å‰§': ['å…¬å¸', 'è€æ¿', 'å‘˜å·¥', 'åˆä½œ', 'ç«äº‰', 'é¡¹ç›®', 'ä¼šè®®', 'è°ˆåˆ¤', 'æŠ•èµ„'],
            'å¤è£…å‰§': ['çš‡å¸', 'å¤§è‡£', 'æœå»·', 'æˆ˜äº‰', 'å°†å†›', 'å£«å…µ', 'ç‹æœ', 'å®«å»·'],
            'éƒ½å¸‚å‰§': ['åŸå¸‚', 'èŒåœº', 'ç™½é¢†', 'ç”Ÿæ´»', 'å·¥ä½œ', 'å‹åŠ›', 'æ¢¦æƒ³', 'å¥‹æ–—']
        }
        
        genre_scores = {}
        for genre, keywords in genre_keywords.items():
            score = sum(1 for keyword in keywords if keyword in content)
            if score > 0:
                genre_scores[genre] = score
        
        if genre_scores:
            return max(genre_scores, key=genre_scores.get)
        return 'é€šç”¨å‰§æƒ…'

    def extract_keywords(self, content: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # å¸¸è§çš„é‡è¦è¯æ±‡
        important_patterns = [
            r'[å››å…­]äºŒå…«æ¡ˆ', r'\d+æ¡ˆ', r'å¬è¯ä¼š', r'ç”³è¯‰', r'å¤å®¡', r'è¯æ®', r'è¯è¯',
            r'æ­£å½“é˜²å«', r'æ•…æ„ä¼¤å®³', r'æ³•å®˜', r'æ£€å¯Ÿå®˜', r'å¾‹å¸ˆ', r'è¾©æŠ¤',
            r'æ®µæ´ªå±±', r'ææ…•æ«', r'å¼ å›­', r'éœ¸å‡Œ', r'æ ¡å›­æš´åŠ›'
        ]
        
        keywords = []
        for pattern in important_patterns:
            matches = re.findall(pattern, content)
            keywords.extend(matches)
        
        return list(set(keywords))  # å»é‡

    def identify_characters(self, subtitles: List[Dict]) -> List[str]:
        """è¯†åˆ«å‰§ä¸­è§’è‰²"""
        # å¸¸è§çš„äººåæ¨¡å¼
        name_patterns = [
            r'[æç‹å¼ åˆ˜é™ˆæ¨èµµé»„å‘¨å´å¾å­™èƒ¡æœ±é«˜æ—ä½•éƒ­é©¬ç½—æ¢å®‹éƒ‘è°¢éŸ©å”å†¯äºè‘£è§ç¨‹æ›¹è¢é‚“è®¸å‚…æ²ˆæ›¾å½­å•è‹å¢è’‹è”¡è´¾ä¸é­è–›å¶é˜ä½™æ½˜æœæˆ´å¤é’Ÿæ±ªç”°ä»»å§œèŒƒæ–¹çŸ³å§šè°­å»–é‚¹ç†Šé‡‘é™†éƒå­”ç™½å´”åº·æ¯›é‚±ç§¦æ±Ÿå²é¡¾ä¾¯é‚µå­Ÿé¾™ä¸‡æ®µé›·é’±æ±¤å°¹é»æ˜“å¸¸æ­¦ä¹”è´ºèµ–é¾šæ–‡][ä¸€-é¾¯]{1,2}'
        ]
        
        characters = set()
        for sub in subtitles:
            for pattern in name_patterns:
                matches = re.findall(pattern, sub['text'])
                characters.update(matches)
        
        # è¿‡æ»¤æ‰å¯èƒ½ä¸æ˜¯äººåçš„è¯
        filtered_characters = []
        for char in characters:
            if len(char) >= 2 and not any(word in char for word in ['æ³•å®˜', 'æ£€å¯Ÿå®˜', 'å¾‹å¸ˆ', 'åŒ»ç”Ÿ', 'æŠ¤å£«']):
                filtered_characters.append(char)
        
        return sorted(filtered_characters)

    def analyze_language_complexity(self, content: str) -> Dict:
        """åˆ†æè¯­è¨€å¤æ‚åº¦"""
        # è®¡ç®—å¥å­é•¿åº¦åˆ†å¸ƒ
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)
        sentence_lengths = [len(s.strip()) for s in sentences if s.strip()]
        
        # è®¡ç®—è¯æ±‡ä¸°å¯Œåº¦
        words = content.split()
        unique_words = set(words)
        vocabulary_richness = len(unique_words) / len(words) if words else 0
        
        # æ£€æµ‹ä¸“ä¸šæœ¯è¯­
        professional_terms = ['æ­£å½“é˜²å«', 'æ•…æ„ä¼¤å®³', 'å¬è¯ä¼š', 'ç”³è¯‰', 'å¤å®¡', 'è¯æ®', 'è¯è¯', 'è¾©æŠ¤', 'èµ·è¯‰']
        term_count = sum(1 for term in professional_terms if term in content)
        
        return {
            'avg_sentence_length': sum(sentence_lengths) / len(sentence_lengths) if sentence_lengths else 0,
            'vocabulary_richness': vocabulary_richness,
            'professional_terms_count': term_count,
            'complexity_level': 'é«˜' if vocabulary_richness > 0.7 and term_count > 5 else ('ä¸­' if vocabulary_richness > 0.5 else 'ä½')
        }

    def generate_detailed_explanation(self, subtitles: List[Dict], analysis: Dict, filename: str) -> str:
        """ç”Ÿæˆè¯¦ç»†ä¸”é€šä¿—æ˜“æ‡‚çš„è§£é‡Š"""
        episode_num = self.extract_episode_number(filename)
        
        explanation = f"""ğŸ“º ç¬¬{episode_num}é›† å­—å¹•å†…å®¹è¯¦ç»†è§£é‡Š
{'=' * 80}

ğŸ“Š åŸºæœ¬ä¿¡æ¯ç»Ÿè®¡
â€¢ å­—å¹•æ¡æ•°ï¼š{analysis['total_subtitles']} æ¡
â€¢ æ€»æ—¶é•¿ï¼š{analysis['total_duration']:.1f} ç§’ ({analysis['total_duration']/60:.1f} åˆ†é’Ÿ)
â€¢ æ€»å­—ç¬¦æ•°ï¼š{analysis['total_characters']} å­—
â€¢ æ€»è¯æ±‡é‡ï¼š{analysis['total_words']} è¯
â€¢ å¹³å‡è¯­é€Ÿï¼š{analysis['average_speaking_rate']:.1f} å­—/ç§’

ğŸ­ å‰§æƒ…ç±»å‹åˆ†æ
â€¢ æ£€æµ‹ç±»å‹ï¼š{analysis['genre']}
â€¢ è¯­è¨€å¤æ‚åº¦ï¼š{analysis['language_complexity']['complexity_level']}
â€¢ ä¸“ä¸šæœ¯è¯­ï¼š{analysis['language_complexity']['professional_terms_count']} ä¸ª
â€¢ è¯æ±‡ä¸°å¯Œåº¦ï¼š{analysis['language_complexity']['vocabulary_richness']:.2f}

ğŸ‘¥ ä¸»è¦è§’è‰²è¯†åˆ«
"""
        if analysis['characters']:
            for char in analysis['characters'][:10]:  # æ˜¾ç¤ºå‰10ä¸ªè§’è‰²
                explanation += f"â€¢ {char}\n"
        else:
            explanation += "â€¢ æœªè¯†åˆ«åˆ°æ˜ç¡®è§’è‰²åç§°\n"

        explanation += f"""
ğŸ”‘ å…³é”®è¯æå–
"""
        if analysis['keywords']:
            for keyword in analysis['keywords'][:15]:  # æ˜¾ç¤ºå‰15ä¸ªå…³é”®è¯
                explanation += f"â€¢ {keyword}\n"
        else:
            explanation += "â€¢ æœªæå–åˆ°ç‰¹å®šå…³é”®è¯\n"

        # åˆ†æç²¾å½©å¯¹è¯ç‰‡æ®µ
        interesting_segments = self.find_interesting_segments(subtitles)
        
        explanation += f"""
ğŸ’¬ ç²¾å½©å¯¹è¯ç‰‡æ®µè§£æï¼ˆå…±{len(interesting_segments)}ä¸ªï¼‰
"""

        for i, segment in enumerate(interesting_segments[:5], 1):  # æ˜¾ç¤ºå‰5ä¸ªç²¾å½©ç‰‡æ®µ
            explanation += f"""
ğŸ“ ç‰‡æ®µ {i}ï¼š{segment['title']}
â° æ—¶é—´ï¼š{segment['start_time']} --> {segment['end_time']} (æ—¶é•¿: {segment['duration']:.1f}ç§’)
ğŸ¯ é‡è¦æ€§ï¼š{segment['importance']}
ğŸ“ å†…å®¹æ‘˜è¦ï¼š{segment['summary']}

ğŸ’­ é€šä¿—è§£é‡Šï¼š
{segment['explanation']}

ğŸ—£ï¸ å…³é”®å¯¹è¯ï¼š
"""
            for dialogue in segment['key_dialogues'][:3]:  # æ¯ä¸ªç‰‡æ®µæ˜¾ç¤ºå‰3å¥å…³é”®å¯¹è¯
                explanation += f"   [{dialogue['time']}] {dialogue['text']}\n"
            
            explanation += "\n"

        # å‰§æƒ…è¿è´¯æ€§åˆ†æ
        explanation += f"""
ğŸ”— å‰§æƒ…è¿è´¯æ€§åˆ†æ
â€¢ å¯¹è¯èŠ‚å¥ï¼š{'å¿«é€Ÿ' if analysis['average_speaking_rate'] > 8 else ('é€‚ä¸­' if analysis['average_speaking_rate'] > 5 else 'ç¼“æ…¢')}
â€¢ æƒ…èŠ‚å¯†åº¦ï¼š{'é«˜å¯†åº¦' if len(interesting_segments) > 8 else ('ä¸­å¯†åº¦' if len(interesting_segments) > 4 else 'ä½å¯†åº¦')}
â€¢ ä¸»çº¿æ¨è¿›ï¼š{self.analyze_plot_progression(subtitles)}

ğŸ“ å†…å®¹æ€»ç»“
{self.generate_content_summary(subtitles, analysis)}

ğŸ’¡ è§‚çœ‹å»ºè®®
{self.generate_viewing_suggestions(analysis)}

ğŸ“Š æŠ€æœ¯ç»Ÿè®¡
â€¢ æœ€é•¿å­—å¹•ï¼š{max([sub['char_count'] for sub in subtitles], default=0)} å­—
â€¢ æœ€çŸ­å­—å¹•ï¼š{min([sub['char_count'] for sub in subtitles if sub['char_count'] > 0], default=0)} å­—
â€¢ å¹³å‡å­—å¹•é•¿åº¦ï¼š{sum([sub['char_count'] for sub in subtitles]) / len(subtitles):.1f} å­—
â€¢ æœ€å¿«è¯­é€Ÿï¼š{max([sub['char_count']/sub['duration'] for sub in subtitles if sub['duration'] > 0], default=0):.1f} å­—/ç§’
â€¢ æœ€æ…¢è¯­é€Ÿï¼š{min([sub['char_count']/sub['duration'] for sub in subtitles if sub['duration'] > 0], default=0):.1f} å­—/ç§’

ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        return explanation

    def find_interesting_segments(self, subtitles: List[Dict]) -> List[Dict]:
        """æŸ¥æ‰¾ç²¾å½©å¯¹è¯ç‰‡æ®µ"""
        segments = []
        
        # å…³é”®è¯æƒé‡
        keywords_weights = {
            'é«˜æƒé‡': ['çœŸç›¸', 'è¯æ®', 'å‘ç°', 'ç§˜å¯†', 'æ­éœ²', 'ç”³è¯‰', 'å¬è¯ä¼š', 'åˆ¤å†³', 'å¤å®¡'],
            'ä¸­æƒé‡': ['æ¡ˆä»¶', 'è°ƒæŸ¥', 'æ³•åº­', 'å¾‹å¸ˆ', 'è¯è¯', 'è¾©æŠ¤', 'èµ·è¯‰', 'å®¡ç†'],
            'ä½æƒé‡': ['é—®é¢˜', 'æƒ…å†µ', 'äº‹æƒ…', 'å¯èƒ½', 'åº”è¯¥', 'è§‰å¾—', 'è®¤ä¸º', 'å¸Œæœ›']
        }
        
        # æƒ…æ„Ÿæ ‡è¯†è¯
        emotion_words = ['æ„¤æ€’', 'ç”Ÿæ°”', 'æ¿€åŠ¨', 'å…´å¥‹', 'ç´§å¼ ', 'å®³æ€•', 'æ‹…å¿ƒ', 'å¼€å¿ƒ', 'é«˜å…´', 'æ‚²ä¼¤', 'éš¾è¿‡', 'éœ‡æƒŠ', 'æƒŠè®¶']
        
        # åˆ†ææ¯ä¸ªå­—å¹•çš„é‡è¦æ€§
        for i, subtitle in enumerate(subtitles):
            score = 0
            text = subtitle['text']
            
            # å…³é”®è¯è¯„åˆ†
            for weight_category, keywords in keywords_weights.items():
                for keyword in keywords:
                    if keyword in text:
                        if weight_category == 'é«˜æƒé‡':
                            score += 3
                        elif weight_category == 'ä¸­æƒé‡':
                            score += 2
                        else:
                            score += 1
            
            # æƒ…æ„Ÿè¯è¯„åˆ†
            for emotion in emotion_words:
                if emotion in text:
                    score += 2
            
            # å¯¹è¯é•¿åº¦è¯„åˆ†
            if subtitle['char_count'] > 30:
                score += 1
            if subtitle['char_count'] > 50:
                score += 1
            
            # è¯­é€Ÿè¯„åˆ†ï¼ˆå¿«è¯­é€Ÿé€šå¸¸è¡¨ç¤ºç´§å¼ æƒ…èŠ‚ï¼‰
            if subtitle['duration'] > 0:
                speaking_rate = subtitle['char_count'] / subtitle['duration']
                if speaking_rate > 10:  # å¿«è¯­é€Ÿ
                    score += 2
            
            if score >= 5:  # é˜ˆå€¼ï¼šè‡³å°‘5åˆ†æ‰ç®—ç²¾å½©
                # æ„å»ºç‰‡æ®µä¸Šä¸‹æ–‡
                context_start = max(0, i - 2)
                context_end = min(len(subtitles), i + 3)
                context_subtitles = subtitles[context_start:context_end]
                
                segment = self.build_segment_info(subtitle, context_subtitles, score, i)
                segments.append(segment)
        
        # æŒ‰é‡è¦æ€§æ’åº
        segments.sort(key=lambda x: x['score'], reverse=True)
        
        # åˆå¹¶ç›¸é‚»ç‰‡æ®µ
        merged_segments = self.merge_adjacent_segments(segments, subtitles)
        
        return merged_segments[:10]  # è¿”å›å‰10ä¸ªæœ€ç²¾å½©çš„ç‰‡æ®µ

    def build_segment_info(self, main_subtitle: Dict, context_subtitles: List[Dict], score: int, index: int) -> Dict:
        """æ„å»ºç‰‡æ®µä¿¡æ¯"""
        # ç”Ÿæˆæ ‡é¢˜
        text = main_subtitle['text']
        if 'çœŸç›¸' in text or 'å‘ç°' in text:
            title = "çœŸç›¸æ­éœ²æ—¶åˆ»"
        elif 'è¯æ®' in text:
            title = "å…³é”®è¯æ®å‘ˆç°"
        elif 'ç”³è¯‰' in text or 'å¬è¯ä¼š' in text:
            title = "æ³•å¾‹ç¨‹åºå…³é”®"
        elif 'æ„¤æ€’' in text or 'ç”Ÿæ°”' in text:
            title = "æƒ…æ„Ÿçˆ†å‘æ—¶åˆ»"
        elif 'è°ƒæŸ¥' in text or 'æ¡ˆä»¶' in text:
            title = "æ¡ˆä»¶è°ƒæŸ¥æ¨è¿›"
        else:
            title = "é‡è¦å¯¹è¯ç‰‡æ®µ"
        
        # ç”Ÿæˆè§£é‡Š
        explanation = self.generate_segment_explanation(main_subtitle, context_subtitles)
        
        # æå–å…³é”®å¯¹è¯
        key_dialogues = []
        for sub in context_subtitles:
            if sub['char_count'] > 15:  # åªæå–æœ‰æ„ä¹‰çš„å¯¹è¯
                key_dialogues.append({
                    'time': sub['start_time'],
                    'text': sub['text']
                })
        
        return {
            'title': title,
            'start_time': context_subtitles[0]['start_time'],
            'end_time': context_subtitles[-1]['end_time'],
            'duration': sum(sub['duration'] for sub in context_subtitles),
            'score': score,
            'importance': self.get_importance_level(score),
            'summary': text[:50] + "..." if len(text) > 50 else text,
            'explanation': explanation,
            'key_dialogues': key_dialogues,
            'index': index
        }

    def generate_segment_explanation(self, main_subtitle: Dict, context_subtitles: List[Dict]) -> str:
        """ç”Ÿæˆç‰‡æ®µçš„é€šä¿—è§£é‡Š"""
        text = main_subtitle['text']
        
        if '428æ¡ˆ' in text or '628æ¡ˆ' in text:
            return "è¿™é‡Œæ¶‰åŠåˆ°æ¡ˆä»¶çš„æ ¸å¿ƒä¿¡æ¯ã€‚428æ¡ˆå’Œ628æ¡ˆæ˜¯å‰§ä¸­çš„é‡è¦æ¡ˆä»¶çº¿ç´¢ï¼Œå…³ç³»åˆ°æ•´ä¸ªæ•…äº‹çš„å‘å±•èµ°å‘ã€‚"
        elif 'ç”³è¯‰' in text:
            return "ç”³è¯‰æ˜¯æ³•å¾‹ç¨‹åºä¸­çš„é‡è¦ç¯èŠ‚ï¼Œæ„å‘³ç€å½“äº‹äººå¯¹åˆ¤å†³ç»“æœä¸æ»¡ï¼Œè¦æ±‚é‡æ–°å®¡ç†ã€‚è¿™é€šå¸¸æ˜¯å‰§æƒ…è½¬æŠ˜çš„å…³é”®æ—¶åˆ»ã€‚"
        elif 'å¬è¯ä¼š' in text:
            return "å¬è¯ä¼šæ˜¯æ³•åº­å®¡ç†çš„æ­£å¼ç¨‹åºï¼Œå„æ–¹å°†åœ¨æ­¤å±•ç¤ºè¯æ®ã€è¿›è¡Œè¾©è®ºã€‚è¿™ç§åœºæ™¯å¾€å¾€æ˜¯å‰§æƒ…çš„é«˜æ½®éƒ¨åˆ†ã€‚"
        elif 'è¯æ®' in text:
            return "è¯æ®æ˜¯æ³•åº­å®¡ç†çš„æ ¸å¿ƒï¼Œæ–°è¯æ®çš„å‡ºç°å¾€å¾€ä¼šæ”¹å˜æ¡ˆä»¶èµ°å‘ï¼Œæ˜¯æ¨åŠ¨å‰§æƒ…å‘å±•çš„é‡è¦å› ç´ ã€‚"
        elif 'æ­£å½“é˜²å«' in text:
            return "æ­£å½“é˜²å«æ˜¯æ³•å¾‹æ¦‚å¿µï¼ŒæŒ‡åœ¨å—åˆ°ä¸æ³•ä¾µå®³æ—¶é‡‡å–çš„åˆç†é˜²å¾¡è¡Œä¸ºã€‚è¿™æ˜¯æ¡ˆä»¶äº‰è®®çš„ç„¦ç‚¹ä¹‹ä¸€ã€‚"
        elif 'éœ¸å‡Œ' in text or 'æ ¡å›­æš´åŠ›' in text:
            return "æ ¡å›­éœ¸å‡Œæ˜¯ç¤¾ä¼šçƒ­ç‚¹é—®é¢˜ï¼Œå‰§ä¸­é€šè¿‡è¿™ä¸€æƒ…èŠ‚åæ˜ ç°å®ä¸­çš„ç¤¾ä¼šé—®é¢˜ï¼Œå¼•å‘è§‚ä¼—æ€è€ƒã€‚"
        elif 'çœŸç›¸' in text or 'å‘ç°' in text:
            return "çœŸç›¸çš„æ­éœ²æ˜¯è§‚ä¼—æœ€æœŸå¾…çš„æ—¶åˆ»ï¼Œè¿™é‡Œå¯èƒ½ä¼šæœ‰é‡è¦çš„æƒ…èŠ‚åè½¬æˆ–å…³é”®ä¿¡æ¯æŠ«éœ²ã€‚"
        elif 'æ®µæ´ªå±±' in text:
            return "æ®µæ´ªå±±æ˜¯å‰§ä¸­é‡è¦è§’è‰²ï¼Œä»–çš„è¨€è¡Œä¸¾æ­¢å¾€å¾€å…³ç³»åˆ°æ¡ˆä»¶çš„å‘å±•å’ŒçœŸç›¸çš„æ­ç¤ºã€‚"
        else:
            return "è¿™æ®µå¯¹è¯åŒ…å«äº†æ¨è¿›å‰§æƒ…çš„é‡è¦ä¿¡æ¯ï¼Œå€¼å¾—è§‚ä¼—ä»”ç»†å…³æ³¨å…¶ä¸­çš„ç»†èŠ‚å’Œå«ä¹‰ã€‚"

    def get_importance_level(self, score: int) -> str:
        """è·å–é‡è¦æ€§ç­‰çº§"""
        if score >= 10:
            return "æå…¶é‡è¦"
        elif score >= 8:
            return "éå¸¸é‡è¦"
        elif score >= 6:
            return "æ¯”è¾ƒé‡è¦"
        else:
            return "ä¸€èˆ¬é‡è¦"

    def merge_adjacent_segments(self, segments: List[Dict], subtitles: List[Dict]) -> List[Dict]:
        """åˆå¹¶ç›¸é‚»çš„ç²¾å½©ç‰‡æ®µ"""
        if not segments:
            return []
        
        merged = []
        current_segment = segments[0].copy()
        
        for next_segment in segments[1:]:
            # æ£€æŸ¥æ˜¯å¦ç›¸é‚»ï¼ˆæ—¶é—´é—´éš”å°äº30ç§’ï¼‰
            current_end = self.time_to_seconds(current_segment['end_time'])
            next_start = self.time_to_seconds(next_segment['start_time'])
            
            if next_start - current_end < 30:  # 30ç§’å†…çš„ç‰‡æ®µåˆå¹¶
                # åˆå¹¶ç‰‡æ®µ
                current_segment['end_time'] = next_segment['end_time']
                current_segment['duration'] = self.time_to_seconds(current_segment['end_time']) - self.time_to_seconds(current_segment['start_time'])
                current_segment['score'] = max(current_segment['score'], next_segment['score'])
                current_segment['key_dialogues'].extend(next_segment['key_dialogues'])
                current_segment['title'] = f"{current_segment['title']} & {next_segment['title']}"
            else:
                merged.append(current_segment)
                current_segment = next_segment.copy()
        
        merged.append(current_segment)
        return merged

    def analyze_plot_progression(self, subtitles: List[Dict]) -> str:
        """åˆ†æå‰§æƒ…æ¨è¿›æƒ…å†µ"""
        content = ' '.join([sub['text'] for sub in subtitles])
        
        if 'ç”³è¯‰' in content and 'å¬è¯ä¼š' in content:
            return "æ³•å¾‹ç¨‹åºæ¨è¿›ï¼Œä»ç”³è¯‰åˆ°å¬è¯ä¼šçš„å®Œæ•´æµç¨‹"
        elif 'è°ƒæŸ¥' in content and 'è¯æ®' in content:
            return "æ¡ˆä»¶è°ƒæŸ¥æ·±å…¥ï¼Œè¯æ®é€æ­¥æ”¶é›†å’Œåˆ†æ"
        elif 'çœŸç›¸' in content or 'å‘ç°' in content:
            return "çœŸç›¸é€æ­¥æ­éœ²ï¼Œå…³é”®ä¿¡æ¯æŠ«éœ²"
        elif 'å®¡ç†' in content or 'æ³•åº­' in content:
            return "æ³•åº­å®¡ç†è¿‡ç¨‹ï¼Œæ³•å¾‹äº‰è®®ç„¦ç‚¹æ˜ç¡®"
        else:
            return "å‰§æƒ…ç¨³æ­¥å‘å±•ï¼Œäººç‰©å…³ç³»å’Œæ•…äº‹çº¿ç´¢æ¨è¿›"

    def generate_content_summary(self, subtitles: List[Dict], analysis: Dict) -> str:
        """ç”Ÿæˆå†…å®¹æ€»ç»“"""
        genre = analysis['genre']
        keywords = analysis['keywords']
        
        summary = f"æœ¬é›†ä¸º{genre}ï¼Œ"
        
        if 'æ³•å¾‹' in genre:
            summary += "ä¸»è¦å›´ç»•æ³•å¾‹æ¡ˆä»¶å±•å¼€ï¼Œæ¶‰åŠæ³•åº­å®¡ç†ã€è¯æ®åˆ†æç­‰æ³•å¾‹ç¨‹åºã€‚"
        elif 'çŠ¯ç½ª' in genre:
            summary += "é‡ç‚¹æè¿°æ¡ˆä»¶è°ƒæŸ¥è¿‡ç¨‹ï¼ŒåŒ…æ‹¬çº¿ç´¢æ”¶é›†ã€çœŸç›¸è¿½æŸ¥ç­‰å†…å®¹ã€‚"
        elif 'çˆ±æƒ…' in genre:
            summary += "èšç„¦äººç‰©æƒ…æ„Ÿå…³ç³»å‘å±•ï¼Œå±•ç°çˆ±æƒ…æ•…äº‹çš„èµ·ä¼å˜åŒ–ã€‚"
        else:
            summary += "é€šè¿‡ä¸°å¯Œçš„äººç‰©å¯¹è¯å’Œæƒ…èŠ‚å‘å±•ï¼Œæ¨è¿›æ•´ä½“æ•…äº‹çº¿ç´¢ã€‚"
        
        if keywords:
            summary += f" å…³é”®è¦ç´ åŒ…æ‹¬ï¼š{', '.join(keywords[:5])}ç­‰ã€‚"
        
        summary += f" è¯­è¨€{analysis['language_complexity']['complexity_level']}ï¼Œé€‚åˆå¯¹{genre.replace('å‰§', '')}é¢˜ææ„Ÿå…´è¶£çš„è§‚ä¼—è§‚çœ‹ã€‚"
        
        return summary

    def generate_viewing_suggestions(self, analysis: Dict) -> str:
        """ç”Ÿæˆè§‚çœ‹å»ºè®®"""
        suggestions = []
        
        # åŸºäºè¯­é€Ÿçš„å»ºè®®
        speaking_rate = analysis['average_speaking_rate']
        if speaking_rate > 8:
            suggestions.append("è¯­é€Ÿè¾ƒå¿«ï¼Œå»ºè®®ä¸“æ³¨è§‚çœ‹ï¼Œä¸è¦åˆ†å¿ƒ")
        elif speaking_rate < 5:
            suggestions.append("è¯­é€Ÿé€‚ä¸­ï¼Œé€‚åˆè½»æ¾è§‚çœ‹")
        
        # åŸºäºå¤æ‚åº¦çš„å»ºè®®
        complexity = analysis['language_complexity']['complexity_level']
        if complexity == 'é«˜':
            suggestions.append("å†…å®¹è¾ƒä¸ºå¤æ‚ï¼Œå»ºè®®ä»”ç»†ç†è§£ä¸“ä¸šæœ¯è¯­")
        elif complexity == 'ä¸­':
            suggestions.append("å†…å®¹é€‚ä¸­ï¼Œä¸€èˆ¬è§‚ä¼—å®¹æ˜“ç†è§£")
        else:
            suggestions.append("å†…å®¹é€šä¿—æ˜“æ‡‚ï¼Œé€‚åˆä¼‘é—²è§‚çœ‹")
        
        # åŸºäºç±»å‹çš„å»ºè®®
        genre = analysis['genre']
        if 'æ³•å¾‹' in genre:
            suggestions.append("æ¶‰åŠæ³•å¾‹ä¸“ä¸šçŸ¥è¯†ï¼Œå¯å…³æ³¨æ³•å¾‹ç¨‹åºå’Œæœ¯è¯­è§£é‡Š")
        elif 'çŠ¯ç½ª' in genre:
            suggestions.append("æ³¨æ„çº¿ç´¢æ”¶é›†è¿‡ç¨‹ï¼Œæ€è€ƒæ¡ˆä»¶é€»è¾‘")
        
        return "ï¼›".join(suggestions) + "ã€‚"

    def extract_episode_number(self, filename: str) -> str:
        """æå–é›†æ•°"""
        match = re.search(r'[Ee](\d+)', filename)
        if match:
            return match.group(1)
        
        match = re.search(r'ç¬¬(\d+)é›†', filename)
        if match:
            return match.group(1)
        
        match = re.search(r'(\d+)', filename)
        if match:
            return match.group(1)
        
        return "æœªçŸ¥"

    def process_single_srt(self, filepath: str) -> bool:
        """å¤„ç†å•ä¸ªSRTæ–‡ä»¶"""
        filename = os.path.basename(filepath)
        print(f"\nğŸ“ åˆ†æ: {filename}")
        
        # è§£æå­—å¹•
        subtitles = self.parse_srt_file(filepath)
        if not subtitles:
            print(f"âŒ å­—å¹•è§£æå¤±è´¥")
            return False
        
        # åˆ†æå†…å®¹
        analysis = self.analyze_subtitle_content(subtitles)
        
        # ç”Ÿæˆè¯¦ç»†è§£é‡Š
        explanation = self.generate_detailed_explanation(subtitles, analysis, filename)
        
        # ä¿å­˜ç»“æœ
        output_filename = f"{os.path.splitext(filename)[0]}_è¯¦ç»†è§£é‡Š.txt"
        output_path = os.path.join(self.output_folder, output_filename)
        
        try:
            platform_fix.safe_file_write(output_path, explanation)
            print(f"âœ… è§£é‡Šæ–‡ä»¶å·²ä¿å­˜: {output_filename}")
            
            # ç”Ÿæˆç®€è¦ç»Ÿè®¡
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   å­—å¹•æ•°é‡: {analysis['total_subtitles']} æ¡")
            print(f"   æ€»æ—¶é•¿: {analysis['total_duration']:.1f} ç§’")
            print(f"   å‰§æƒ…ç±»å‹: {analysis['genre']}")
            print(f"   ç²¾å½©ç‰‡æ®µ: {len(self.find_interesting_segments(subtitles))} ä¸ª")
            
            return True
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            return False

    def process_all_srt_files(self):
        """å¤„ç†æ‰€æœ‰SRTæ–‡ä»¶"""
        print("\nğŸš€ å¼€å§‹æ‰¹é‡SRTå­—å¹•åˆ†æ")
        print("=" * 50)
        
        # è·å–æ‰€æœ‰SRTæ–‡ä»¶
        srt_files = []
        for file in os.listdir(self.srt_folder):
            if file.endswith(('.srt', '.txt')) and not file.startswith('.'):
                srt_files.append(os.path.join(self.srt_folder, file))
        
        if not srt_files:
            print(f"âŒ {self.srt_folder}/ ç›®å½•ä¸­æœªæ‰¾åˆ°SRTæ–‡ä»¶")
            return
        
        srt_files.sort()  # æŒ‰æ–‡ä»¶åæ’åº
        print(f"ğŸ“ æ‰¾åˆ° {len(srt_files)} ä¸ªå­—å¹•æ–‡ä»¶")
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        success_count = 0
        for srt_file in srt_files:
            try:
                if self.process_single_srt(srt_file):
                    success_count += 1
            except Exception as e:
                print(f"âŒ å¤„ç† {os.path.basename(srt_file)} å¤±è´¥: {e}")
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.generate_summary_report(srt_files, success_count)
        
        print(f"\nğŸ“Š å¤„ç†å®Œæˆ:")
        print(f"âœ… æˆåŠŸåˆ†æ: {success_count}/{len(srt_files)} ä¸ªæ–‡ä»¶")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {self.output_folder}/ ç›®å½•")

    def generate_summary_report(self, srt_files: List[str], success_count: int):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        report = f"""ğŸ“º SRTå­—å¹•åˆ†ææ€»ç»“æŠ¥å‘Š
{'=' * 80}

ğŸ“Š å¤„ç†ç»Ÿè®¡
â€¢ æ€»æ–‡ä»¶æ•°ï¼š{len(srt_files)} ä¸ª
â€¢ æˆåŠŸåˆ†æï¼š{success_count} ä¸ª
â€¢ å¤±è´¥æ•°é‡ï¼š{len(srt_files) - success_count} ä¸ª
â€¢ æˆåŠŸç‡ï¼š{success_count/len(srt_files)*100:.1f}%

ğŸ“ è¾“å‡ºæ–‡ä»¶
"""
        
        # åˆ—å‡ºæ‰€æœ‰è¾“å‡ºæ–‡ä»¶
        output_files = [f for f in os.listdir(self.output_folder) if f.endswith('.txt')]
        for output_file in sorted(output_files):
            report += f"â€¢ {output_file}\n"
        
        report += f"""
ğŸ¯ åˆ†æç‰¹è‰²
â€¢ ç²¾å‡†SRTå­—å¹•è§£æï¼Œæ™ºèƒ½é”™è¯¯ä¿®æ­£
â€¢ è¯¦ç»†å‰§æƒ…ç±»å‹è¯†åˆ«å’Œè§’è‰²åˆ†æ
â€¢ é€šä¿—æ˜“æ‡‚çš„å†…å®¹è§£é‡Šå’Œè§‚çœ‹å»ºè®®
â€¢ ç²¾å½©ç‰‡æ®µè‡ªåŠ¨è¯†åˆ«å’Œé‡ç‚¹æ ‡æ³¨
â€¢ è¯­è¨€å¤æ‚åº¦å’ŒæŠ€æœ¯ç»Ÿè®¡åˆ†æ

ğŸ’¡ ä½¿ç”¨è¯´æ˜
æ¯ä¸ªè§£é‡Šæ–‡ä»¶åŒ…å«ï¼š
1. åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ—¶é•¿ã€å­—æ•°ã€è¯­é€Ÿç­‰ï¼‰
2. å‰§æƒ…ç±»å‹å’Œè§’è‰²è¯†åˆ«
3. ç²¾å½©å¯¹è¯ç‰‡æ®µè¯¦ç»†è§£æ
4. é€šä¿—æ˜“æ‡‚çš„å†…å®¹è§£é‡Š
5. ä¸ªæ€§åŒ–è§‚çœ‹å»ºè®®

ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        report_path = os.path.join(self.output_folder, "åˆ†ææ€»ç»“æŠ¥å‘Š.txt")
        platform_fix.safe_file_write(report_path, report)
        print(f"ğŸ“‹ æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: åˆ†ææ€»ç»“æŠ¥å‘Š.txt")

    def show_main_menu(self):
        """æ˜¾ç¤ºä¸»èœå•"""
        while True:
            print("\n" + "=" * 50)
            print("ğŸ“ SRTå­—å¹•ç²¾å‡†åˆ†æè§£é‡Šç³»ç»Ÿ")
            print("=" * 50)
            
            print("\nğŸ¯ ä¸»è¦åŠŸèƒ½:")
            print("1. ğŸ“– åˆ†æå•ä¸ªSRTæ–‡ä»¶")
            print("2. ğŸ“š æ‰¹é‡åˆ†ææ‰€æœ‰SRTæ–‡ä»¶")
            print("3. ğŸ“ æŸ¥çœ‹æ–‡ä»¶çŠ¶æ€")
            print("4. ğŸ”§ æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ")
            print("0. âŒ é€€å‡ºç³»ç»Ÿ")
            
            try:
                choice = input("\nè¯·é€‰æ‹©æ“ä½œ (0-4): ").strip()
                
                if choice == '1':
                    self.analyze_single_file()
                elif choice == '2':
                    self.process_all_srt_files()
                elif choice == '3':
                    self.show_file_status()
                elif choice == '4':
                    self.check_system_status()
                elif choice == '0':
                    print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨SRTå­—å¹•åˆ†æç³»ç»Ÿï¼")
                    break
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥0-4")
                    
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­")
                break
            except Exception as e:
                print(f"âŒ æ“ä½œé”™è¯¯: {e}")

    def analyze_single_file(self):
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        srt_files = [f for f in os.listdir(self.srt_folder) 
                    if f.endswith(('.srt', '.txt')) and not f.startswith('.')]
        
        if not srt_files:
            print(f"âŒ {self.srt_folder}/ ç›®å½•ä¸­æœªæ‰¾åˆ°SRTæ–‡ä»¶")
            return
        
        print(f"\nğŸ“ å¯ç”¨çš„SRTæ–‡ä»¶:")
        for i, file in enumerate(srt_files, 1):
            print(f"{i}. {file}")
        
        try:
            choice = int(input(f"\nè¯·é€‰æ‹©æ–‡ä»¶ (1-{len(srt_files)}): "))
            if 1 <= choice <= len(srt_files):
                filepath = os.path.join(self.srt_folder, srt_files[choice-1])
                self.process_single_srt(filepath)
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")

    def show_file_status(self):
        """æ˜¾ç¤ºæ–‡ä»¶çŠ¶æ€"""
        srt_files = [f for f in os.listdir(self.srt_folder) if f.endswith(('.srt', '.txt'))]
        output_files = [f for f in os.listdir(self.output_folder) if f.endswith('.txt')]
        
        print(f"\nğŸ“Š æ–‡ä»¶çŠ¶æ€:")
        print(f"ğŸ“ SRTå­—å¹•æ–‡ä»¶: {len(srt_files)} ä¸ª")
        for f in srt_files[:5]:
            print(f"   â€¢ {f}")
        if len(srt_files) > 5:
            print(f"   â€¢ ... è¿˜æœ‰ {len(srt_files)-5} ä¸ªæ–‡ä»¶")
        
        print(f"ğŸ“„ åˆ†æç»“æœæ–‡ä»¶: {len(output_files)} ä¸ª")
        for f in output_files[:5]:
            print(f"   â€¢ {f}")
        if len(output_files) > 5:
            print(f"   â€¢ ... è¿˜æœ‰ {len(output_files)-5} ä¸ªæ–‡ä»¶")

    def check_system_status(self):
        """æ£€æŸ¥ç³»ç»ŸçŠ¶æ€"""
        print(f"\nğŸ”§ ç³»ç»ŸçŠ¶æ€æ£€æŸ¥:")
        print(f"ğŸ“ å­—å¹•ç›®å½•: {self.srt_folder}/ {'âœ… å­˜åœ¨' if os.path.exists(self.srt_folder) else 'âŒ ä¸å­˜åœ¨'}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_folder}/ {'âœ… å­˜åœ¨' if os.path.exists(self.output_folder) else 'âŒ ä¸å­˜åœ¨'}")
        print(f"ğŸ’¾ ç¼“å­˜ç›®å½•: {self.cache_folder}/ {'âœ… å­˜åœ¨' if os.path.exists(self.cache_folder) else 'âŒ ä¸å­˜åœ¨'}")
        print(f"ğŸ¤– AIé…ç½®: {'âœ… å·²é…ç½®' if self.ai_config.get('enabled') else 'âŒ æœªé…ç½®'}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        analyzer = SRTAnalyzerOnly()
        analyzer.show_main_menu()
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
