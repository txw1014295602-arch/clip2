
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ™ºèƒ½å­—å¹•åˆ†æå™¨ - å¤šå‰§æƒ…ç±»å‹è‡ªé€‚åº”åˆ†æ
"""

import os
import re
import json
import requests
from typing import List, Dict, Tuple, Optional
from datetime import datetime
from clip_rules import ClipRules

# å¹³å°å…¼å®¹æ€§ä¿®å¤
try:
    from platform_fix import fix_encoding, safe_file_read, safe_file_write
    fix_encoding()
except ImportError:
    def safe_file_read(filepath, encoding='utf-8'):
        with open(filepath, 'r', encoding=encoding, errors='ignore') as f:
            return f.read()

    def safe_file_write(filepath, content, encoding='utf-8'):
        with open(filepath, 'w', encoding=encoding, errors='ignore') as f:
            f.write(content)

class IntelligentSubtitleAnalyzer:
    def __init__(self, use_ai_analysis: bool = True, api_key: Optional[str] = None):
        self.use_ai_analysis = use_ai_analysis
        self.api_key = api_key

        # æ™ºèƒ½å‰§æƒ…ç±»å‹è¯†åˆ«
        self.genre_patterns = {
            'legal': {
                'keywords': ['æ³•å®˜', 'æ£€å¯Ÿå®˜', 'å¾‹å¸ˆ', 'æ³•åº­', 'å®¡åˆ¤', 'è¯æ®', 'æ¡ˆä»¶', 'èµ·è¯‰', 'è¾©æŠ¤', 'åˆ¤å†³', 'ç”³è¯‰', 'å¬è¯ä¼š'],
                'weight': 1.0
            },
            'crime': {
                'keywords': ['è­¦å¯Ÿ', 'çŠ¯ç½ª', 'å«Œç–‘äºº', 'è°ƒæŸ¥', 'ç ´æ¡ˆ', 'çº¿ç´¢', 'å‡¶æ‰‹', 'æ¡ˆå‘', 'ä¾¦æ¢', 'åˆ‘ä¾¦', 'è¿½è¸ª', 'é€®æ•'],
                'weight': 1.0
            },
            'medical': {
                'keywords': ['åŒ»ç”Ÿ', 'æŠ¤å£«', 'åŒ»é™¢', 'æ‰‹æœ¯', 'ç—…äºº', 'è¯Šæ–­', 'æ²»ç–—', 'ç—…æƒ…', 'æ€¥è¯Š', 'æ•‘æŠ¤è½¦', 'è¯ç‰©', 'ç—…æˆ¿'],
                'weight': 1.0
            },
            'romance': {
                'keywords': ['çˆ±æƒ…', 'å–œæ¬¢', 'å¿ƒåŠ¨', 'è¡¨ç™½', 'çº¦ä¼š', 'åˆ†æ‰‹', 'å¤åˆ', 'ç»“å©š', 'æƒ…ä¾£', 'æ‹äºº', 'æš—æ‹', 'åˆæ‹'],
                'weight': 1.0
            },
            'family': {
                'keywords': ['å®¶åº­', 'çˆ¶æ¯', 'å­©å­', 'å…„å¼Ÿ', 'å§å¦¹', 'äº²æƒ…', 'å®¶äºº', 'å›¢èš', 'ç¦»åˆ«', 'æˆé•¿', 'æ•™è‚²', 'ä»£æ²Ÿ'],
                'weight': 1.0
            },
            'business': {
                'keywords': ['å…¬å¸', 'è€æ¿', 'å‘˜å·¥', 'åˆä½œ', 'ç«äº‰', 'é¡¹ç›®', 'ä¼šè®®', 'è°ˆåˆ¤', 'æŠ•èµ„', 'åˆ›ä¸š', 'èŒåœº', 'æ™‹å‡'],
                'weight': 1.0
            },
            'historical': {
                'keywords': ['çš‡å¸', 'å¤§è‡£', 'æœå»·', 'æˆ˜äº‰', 'å°†å†›', 'å£«å…µ', 'ç‹æœ', 'å®«å»·', 'æ”¿æ²»', 'æƒåŠ›', 'å›ä¹±', 'èµ·ä¹‰'],
                'weight': 1.0
            },
            'fantasy': {
                'keywords': ['é­”æ³•', 'æ­¦åŠŸ', 'ä¿®ç‚¼', 'ä»™äºº', 'å¦–æ€ª', 'ç¥è¯', 'ä¼ è¯´', 'æ³•æœ¯', 'çµåŠ›', 'å¼‚èƒ½', 'ç©¿è¶Š', 'é‡ç”Ÿ'],
                'weight': 1.0
            }
        }

        # é€šç”¨æˆå‰§å¼ åŠ›æ ‡è¯†è¯
        self.universal_dramatic_keywords = [
            'çªç„¶', 'å¿½ç„¶', 'æ²¡æƒ³åˆ°', 'åŸæ¥', 'å±…ç„¶', 'ç«Ÿç„¶', 'éœ‡æƒŠ', 'æƒŠè®¶', 'æ„å¤–', 'å‘ç°',
            'çœŸç›¸', 'ç§˜å¯†', 'éšç’', 'æ­éœ²', 'æš´éœ²', 'çˆ†å‘', 'å†²çª', 'å¯¹æŠ—', 'äº‰åµ', 'åè½¬',
            'å±é™©', 'ç´§æ€¥', 'æ•‘å‘½', 'å¸®åŠ©', 'æ‹¯æ•‘', 'é€ƒè·‘', 'è¿½é€', 'æ‰“æ–—', 'ç”Ÿæ­»', 'å…³é”®'
        ]

        # é€šç”¨æƒ…æ„Ÿçˆ†å‘ç‚¹æ ‡è¯†
        self.universal_emotional_markers = [
            'å“­', 'ç¬‘', 'å–Š', 'å«', 'æ€’', 'æ°”', 'æ¿€åŠ¨', 'å…´å¥‹', 'ç´§å¼ ', 'å®³æ€•', 'æ‹…å¿ƒ', 'ç„¦è™‘',
            'å¼€å¿ƒ', 'é«˜å…´', 'æ‚²ä¼¤', 'éš¾è¿‡', 'ç—›è‹¦', 'ç»æœ›', 'å¸Œæœ›', 'æ„ŸåŠ¨', 'æ¸©æš–', 'å¤±æœ›',
            'æ„¤æ€’', 'ç”Ÿæ°”', 'æš´æ€’', 'å´©æºƒ', 'é¢¤æŠ–', 'å¿ƒè·³', 'å‘¼å¸', 'çœ¼æ³ª', 'å¾®ç¬‘', 'æ‹¥æŠ±'
        ]

        # å¯¹è¯å¼ºåº¦æ ‡è¯†
        self.dialogue_intensity_markers = [
            'ï¼', 'ï¼Ÿ', '...', 'å•Š', 'å‘€', 'å“¦', 'å”‰', 'å“', 'å—¯', 'å‘¢', 'å§', 'å—', 'å’¦', 'å“‡'
        ]

        # åŠ¨æ€è°ƒæ•´æƒé‡
        self.adaptive_weights = {
            'genre_match': 5.0,
            'dramatic_tension': 3.0,
            'emotional_intensity': 2.0,
            'dialogue_richness': 1.5,
            'character_development': 2.5,
            'plot_advancement': 4.0
        }

        # è‡ªåŠ¨è¯†åˆ«çš„å‰§æƒ…ç±»å‹
        self.detected_genre = None
        self.genre_confidence = 0.0

    def detect_genre(self, all_subtitles: List[Dict]) -> str:
        """æ™ºèƒ½è¯†åˆ«å‰§æƒ…ç±»å‹"""
        genre_scores = {}
        total_text = " ".join([sub['text'] for sub in all_subtitles[:200]])  # åˆ†æå‰200æ¡å­—å¹•

        for genre, pattern in self.genre_patterns.items():
            score = 0
            for keyword in pattern['keywords']:
                score += total_text.count(keyword) * pattern['weight']
            genre_scores[genre] = score

        # æ‰¾å‡ºæœ€åŒ¹é…çš„ç±»å‹
        if genre_scores:
            best_genre = max(genre_scores, key=genre_scores.get)
            max_score = genre_scores[best_genre]

            if max_score > 5:  # è¶³å¤Ÿçš„åŒ¹é…åº¦
                self.detected_genre = best_genre
                self.genre_confidence = min(max_score / 50, 1.0)  # å½’ä¸€åŒ–ä¿¡å¿ƒåº¦
                return best_genre

        return 'general'  # é€šç”¨ç±»å‹

    def get_genre_specific_keywords(self, genre: str) -> List[str]:
        """è·å–ç‰¹å®šç±»å‹çš„å…³é”®è¯"""
        if genre in self.genre_patterns:
            return self.genre_patterns[genre]['keywords']
        return []

    def parse_subtitle_file(self, filepath: str) -> List[Dict]:
        """è§£æå­—å¹•æ–‡ä»¶ï¼Œè‡ªåŠ¨ä¿®æ­£å¸¸è§é”™åˆ«å­—"""
        content = safe_file_read(filepath)

        # é€šç”¨é”™åˆ«å­—ä¿®æ­£
        corrections = {
            'é˜²è¡›': 'é˜²å«', 'æ­£ç•¶': 'æ­£å½“', 'è¨¼æ“š': 'è¯æ®', 'æª¢å¯Ÿå®˜': 'æ£€å¯Ÿå®˜',
            'ç™¼ç¾': 'å‘ç°', 'è¨­è¨ˆ': 'è®¾è®¡', 'é–‹å§‹': 'å¼€å§‹', 'çµæŸ': 'ç»“æŸ',
            'å•é¡Œ': 'é—®é¢˜', 'æ©Ÿæœƒ': 'æœºä¼š', 'æ±ºå®š': 'å†³å®š', 'é¸æ“‡': 'é€‰æ‹©'
        }

        for old, new in corrections.items():
            content = content.replace(old, new)

        # åˆ†å‰²å­—å¹•å—
        blocks = re.split(r'\n\s*\n', content.strip())
        subtitles = []

        for block in blocks:
            lines = block.strip().split('\n')
            if len(lines) >= 3:
                try:
                    index = int(lines[0])
                    time_match = re.match(r'(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})', lines[1])
                    if time_match:
                        start_time = time_match.group(1)
                        end_time = time_match.group(2)
                        text = '\n'.join(lines[2:])

                        subtitles.append({
                            'index': index,
                            'start': start_time,
                            'end': end_time,
                            'text': text,
                            'episode': os.path.basename(filepath)
                        })
                except (ValueError, IndexError):
                    continue

        return subtitles

    def calculate_intelligent_score(self, text: str, context: Dict = None) -> float:
        """æ™ºèƒ½è¯„åˆ†ç®—æ³• - è‡ªé€‚åº”ä¸åŒå‰§æƒ…ç±»å‹"""

        # åŸºç¡€è§„åˆ™è¯„åˆ†
        rule_score = self._calculate_adaptive_rule_score(text, context)

        # å¦‚æœå¯ç”¨AIåˆ†æï¼Œè·å–AIè¯„åˆ†
        if self.use_ai_analysis:
            ai_score = self._calculate_ai_score(text, context)
            if ai_score > 0:
                # åŠ¨æ€æ··åˆè¯„åˆ†
                rule_weight = 0.4 if self.genre_confidence > 0.7 else 0.6
                ai_weight = 1 - rule_weight
                final_score = rule_score * rule_weight + ai_score * ai_weight
                return final_score

        return rule_score

    def _calculate_adaptive_rule_score(self, text: str, context: Dict = None) -> float:
        """è‡ªé€‚åº”è§„åˆ™è¯„åˆ†"""
        score = 0

        # ç±»å‹åŒ¹é…è¯„åˆ†
        if self.detected_genre:
            genre_keywords = self.get_genre_specific_keywords(self.detected_genre)
            for keyword in genre_keywords:
                if keyword in text:
                    score += self.adaptive_weights['genre_match'] * self.genre_confidence

        # é€šç”¨æˆå‰§å¼ åŠ›è¯„åˆ†
        for keyword in self.universal_dramatic_keywords:
            if keyword in text:
                score += self.adaptive_weights['dramatic_tension']

        # æƒ…æ„Ÿå¼ºåº¦è¯„åˆ†
        for emotion in self.universal_emotional_markers:
            if emotion in text:
                score += self.adaptive_weights['emotional_intensity']

        # å¯¹è¯ä¸°å¯Œåº¦
        for marker in self.dialogue_intensity_markers:
            score += text.count(marker) * self.adaptive_weights['dialogue_richness']

        # æ–‡æœ¬é•¿åº¦å’Œè´¨é‡
        text_length = len(text)
        if 15 <= text_length <= 200:
            score += 2
        elif text_length > 200:
            score += 1

        # è§’è‰²å‘å±•æ ‡è¯†
        character_indicators = ['å†³å®š', 'é€‰æ‹©', 'æ”¹å˜', 'æˆé•¿', 'é¢†æ‚Ÿ', 'æ˜ç™½', 'åšæŒ', 'æ”¾å¼ƒ']
        for indicator in character_indicators:
            if indicator in text:
                score += self.adaptive_weights['character_development']

        # å‰§æƒ…æ¨è¿›æ ‡è¯†
        plot_indicators = ['ç„¶å', 'æ¥ç€', 'éšå', 'çªç„¶', 'è¿™æ—¶', 'ç»“æœ', 'æœ€ç»ˆ', 'ç»ˆäº']
        for indicator in plot_indicators:
            if indicator in text:
                score += self.adaptive_weights['plot_advancement']

        return score

    def _calculate_ai_score(self, text: str, context: Dict = None) -> float:
        """ä½¿ç”¨AIæ¨¡å‹è¯„ä¼°å‰§æƒ…é‡è¦æ€§ - æ”¯æŒå¤šç§æ¨¡å‹"""
        try:
            from api_config_helper import config_helper
            
            config = config_helper.load_config()
            if not config.get('enabled') or not config.get('api_key'):
                return 0.0

            if len(text) < 10:
                return 0.0
            if len(text) > 500:
                text = text[:500] + "..."

            # åŠ¨æ€ç”Ÿæˆæç¤ºè¯
            genre_context = f"è¿™æ˜¯ä¸€éƒ¨{self.detected_genre}ç±»å‹çš„ç”µè§†å‰§" if self.detected_genre else "è¿™æ˜¯ä¸€éƒ¨ç”µè§†å‰§"

            prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„ç”µè§†å‰§å‰ªè¾‘å¸ˆï¼Œä¸“æ³¨äºè¯†åˆ«ç²¾å½©ç‰‡æ®µã€‚

{genre_context}ï¼Œè¯·è¯„ä¼°ä»¥ä¸‹å¯¹è¯ç‰‡æ®µçš„å‰ªè¾‘ä»·å€¼ï¼š
"{text}"

è¯„ä¼°æ ‡å‡†ï¼š
1. å‰§æƒ…é‡è¦æ€§(0-2åˆ†)ï¼šæ˜¯å¦æ¨è¿›ä¸»è¦æ•…äº‹çº¿ï¼ŒåŒ…å«å…³é”®ä¿¡æ¯
2. æˆå‰§å¼ åŠ›(0-2åˆ†)ï¼šæ˜¯å¦åŒ…å«å†²çªã€è½¬æŠ˜ã€æ„å¤–å‘ç°
3. æƒ…æ„Ÿå…±é¸£(0-2åˆ†)ï¼šæ˜¯å¦å¼•å‘è§‚ä¼—æƒ…æ„Ÿååº”ï¼Œæœ‰æ„Ÿäººæˆ–éœ‡æ’¼æ—¶åˆ»
4. è§’è‰²å‘å±•(0-2åˆ†)ï¼šæ˜¯å¦å±•ç°è§’è‰²æˆé•¿ã€å…³ç³»å˜åŒ–ã€é‡è¦å†³å®š
5. è§‚ä¼—å¸å¼•åŠ›(0-2åˆ†)ï¼šæ˜¯å¦åˆ¶é€ æ‚¬å¿µã€å¹½é»˜ã€ç´§å¼ æ„Ÿ

è¯·æ ¹æ®ä»¥ä¸Šæ ‡å‡†ç»™å‡º0-10åˆ†çš„ç»¼åˆè¯„åˆ†ã€‚
åªéœ€è¦è¾“å‡ºæ•°å­—ï¼Œä¾‹å¦‚ï¼š8.5"""

            # æ ¹æ®ä¸åŒAPIæä¾›å•†æ„å»ºè¯·æ±‚
            provider = config.get('provider', 'openai')
            
            if provider == 'gemini':
                return self._call_gemini_api(config, prompt)
            elif provider == 'qwen':
                return self._call_qwen_api(config, prompt)
            elif provider == 'doubao':
                return self._call_doubao_api(config, prompt)
            else:
                return self._call_standard_api(config, prompt)

        except Exception as e:
            return 0.0

    def _call_gemini_api(self, config: Dict, prompt: str) -> float:
        """è°ƒç”¨Gemini API"""
        try:
            url = f"https://generativelanguage.googleapis.com/v1/models/{config['model']}:generateContent?key={config['api_key']}"
            
            data = {
                "contents": [{
                    "parts": [{"text": prompt}]
                }],
                "generationConfig": {
                    "maxOutputTokens": 20,
                    "temperature": 0.2
                }
            }
            
            response = requests.post(url, json=data, timeout=10)
            
            if response.status_code == 200:
                result = response.json()
                text = result.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '')
                score_match = re.search(r'(\d+\.?\d*)', text.strip())
                if score_match:
                    return min(max(float(score_match.group(1)), 0), 10)
            
            return 0.0
            
        except Exception:
            return 0.0

    def _call_qwen_api(self, config: Dict, prompt: str) -> float:
        """è°ƒç”¨é€šä¹‰åƒé—®API"""
        try:
            headers = {
                'Authorization': f'Bearer {config["api_key"]}',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': config['model'],
                'input': {'prompt': prompt},
                'parameters': {
                    'max_tokens': 20,
                    'temperature': 0.2
                }
            }
            
            response = requests.post(config['url'], headers=headers, json=data, timeout=10)
            
            if response.status_code == 200:
                result = response.json()
                text = result.get('output', {}).get('text', '').strip()
                score_match = re.search(r'(\d+\.?\d*)', text)
                if score_match:
                    return min(max(float(score_match.group(1)), 0), 10)
            
            return 0.0
            
        except Exception:
            return 0.0

    def _call_doubao_api(self, config: Dict, prompt: str) -> float:
        """è°ƒç”¨è±†åŒ…API"""
        try:
            headers = {
                'Authorization': f'Bearer {config["api_key"]}',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': config['model'],
                'messages': [
                    {'role': 'system', 'content': 'ä½ æ˜¯ä¸“ä¸šçš„ç”µè§†å‰§å‰ªè¾‘å¸ˆï¼Œä¸“æ³¨äºè¯„ä¼°ç‰‡æ®µçš„å‰ªè¾‘ä»·å€¼ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¯„åˆ†æ ‡å‡†ç»™å‡º0-10åˆ†çš„æ•°å­—è¯„åˆ†ã€‚'},
                    {'role': 'user', 'content': prompt}
                ],
                'max_tokens': 20,
                'temperature': 0.2
            }
            
            response = requests.post(config['url'], headers=headers, json=data, timeout=10)
            
            if response.status_code == 200:
                result = response.json()
                text = result.get('choices', [{}])[0].get('message', {}).get('content', '').strip()
                score_match = re.search(r'(\d+\.?\d*)', text)
                if score_match:
                    return min(max(float(score_match.group(1)), 0), 10)
            
            return 0.0
            
        except Exception:
            return 0.0

    def _call_standard_api(self, config: Dict, prompt: str) -> float:
        """è°ƒç”¨æ ‡å‡†OpenAIæ ¼å¼API"""
        try:
            headers = {
                'Authorization': f'Bearer {config["api_key"]}',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': config['model'],
                'messages': [
                    {'role': 'system', 'content': 'ä½ æ˜¯ä¸“ä¸šçš„ç”µè§†å‰§å‰ªè¾‘å¸ˆï¼Œä¸“æ³¨äºè¯„ä¼°ç‰‡æ®µçš„å‰ªè¾‘ä»·å€¼ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¯„åˆ†æ ‡å‡†ç»™å‡º0-10åˆ†çš„æ•°å­—è¯„åˆ†ã€‚'},
                    {'role': 'user', 'content': prompt}
                ],
                'max_tokens': 20,
                'temperature': 0.2
            }
            
            response = requests.post(config['url'], headers=headers, json=data, timeout=10)
            
            if response.status_code == 200:
                result = response.json()
                text = result.get('choices', [{}])[0].get('message', {}).get('content', '').strip()
                score_match = re.search(r'(\d+\.?\d*)', text)
                if score_match:
                    return min(max(float(score_match.group(1)), 0), 10)
            else:
                print(f"âš  APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            
            return 0.0
            
        except requests.exceptions.RequestException as e:
            print(f"âš  ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
            return 0.0
        except Exception as e:
            print(f"âš  APIè°ƒç”¨å¼‚å¸¸: {e}")
            return 0.0

    def identify_smart_segments(self, subtitles: List[Dict]) -> List[Dict]:
        """æ™ºèƒ½è¯†åˆ«ç²¾å½©ç‰‡æ®µ"""
        high_relevance_segments = []

        # æ™ºèƒ½é˜ˆå€¼è°ƒæ•´ - é’ˆå¯¹çŸ­è§†é¢‘ä¼˜åŒ–
        base_threshold = 5.0  # é™ä½åŸºç¡€é˜ˆå€¼ï¼Œè·å–æ›´å¤šç²¾å½©ç‰‡æ®µ
        if self.detected_genre in ['crime', 'legal']:
            threshold = base_threshold - 1.5  # æ³•å¾‹å‰§ç²¾å½©ç‰‡æ®µè¾ƒå¤š
        elif self.detected_genre in ['romance', 'family']:
            threshold = base_threshold - 1.0  # æƒ…æ„Ÿå‰§ä¹Ÿéœ€è¦æ›´å¤šç‰‡æ®µ
        else:
            threshold = base_threshold

        for i, subtitle in enumerate(subtitles):
            # æ„å»ºä¸Šä¸‹æ–‡
            context = {
                'position': i / len(subtitles),  # åœ¨å‰§é›†ä¸­çš„ä½ç½®
                'prev_text': subtitles[i-1]['text'] if i > 0 else '',
                'next_text': subtitles[i+1]['text'] if i < len(subtitles)-1 else '',
                'genre': self.detected_genre
            }

            score = self.calculate_intelligent_score(subtitle['text'], context)
            if score >= threshold:
                high_relevance_segments.append({
                    'index': i,
                    'subtitle': subtitle,
                    'score': score,
                    'context': context
                })

        # æŒ‰åˆ†æ•°æ’åº
        high_relevance_segments.sort(key=lambda x: x['score'], reverse=True)

        # æ™ºèƒ½ç‰‡æ®µåˆå¹¶
        coherent_segments = self._merge_coherent_segments(subtitles, high_relevance_segments)

        return coherent_segments

    def _merge_coherent_segments(self, subtitles: List[Dict], high_segments: List[Dict]) -> List[Dict]:
        """æ™ºèƒ½åˆå¹¶è¿è´¯ç‰‡æ®µ"""
        coherent_segments = []
        used_indices = set()

        for segment_data in high_segments:
            start_idx = segment_data['index']
            if start_idx in used_indices:
                continue

            # åŠ¨æ€æ‰©å±•èŒƒå›´
            expand_range = 12 if self.detected_genre in ['romance', 'family'] else 10

            segment_start = max(0, start_idx - expand_range)
            segment_end = min(len(subtitles) - 1, start_idx + expand_range)

            # å¯»æ‰¾è‡ªç„¶è¾¹ç•Œ
            segment_start = self._find_natural_start(subtitles, segment_start, start_idx)
            segment_end = self._find_natural_end(subtitles, start_idx, segment_end)

            # è®¡ç®—æ—¶é•¿
            start_time = self.time_to_seconds(subtitles[segment_start]['start'])
            end_time = self.time_to_seconds(subtitles[segment_end]['end'])
            duration = end_time - start_time

            # åŠ¨æ€æ—¶é•¿æ§åˆ¶
            min_duration = 90 if self.detected_genre in ['romance', 'family'] else 120
            max_duration = 200 if self.detected_genre in ['crime', 'legal'] else 180

            if min_duration <= duration <= max_duration:
                coherent_segments.append({
                    'start_index': segment_start,
                    'end_index': segment_end,
                    'start_time': subtitles[segment_start]['start'],
                    'end_time': subtitles[segment_end]['end'],
                    'duration': duration,
                    'core_content': self._extract_smart_content(subtitles, segment_start, segment_end),
                    'key_dialogue': self._extract_key_dialogue(subtitles, segment_start, segment_end),
                    'significance': self._analyze_smart_significance(subtitles, segment_start, segment_end),
                    'emotional_tone': self._analyze_emotional_tone(subtitles, segment_start, segment_end),
                    'genre': self.detected_genre,
                    'confidence': segment_data['score']
                })

                # æ ‡è®°å·²ä½¿ç”¨çš„ç´¢å¼•
                for idx in range(segment_start, segment_end + 1):
                    used_indices.add(idx)

        return coherent_segments[:3]  # æ¯é›†æœ€å¤š3ä¸ªæ ¸å¿ƒç‰‡æ®µ

    def _find_natural_start(self, subtitles: List[Dict], search_start: int, anchor: int) -> int:
        """å¯»æ‰¾è‡ªç„¶å¼€å§‹ç‚¹"""
        scene_starters = ['çªç„¶', 'è¿™æ—¶', 'å¿½ç„¶', 'åˆšæ‰', 'ç°åœ¨', 'é‚£å¤©', 'å½“æ—¶', 'éšå³', 'åæ¥', 'æ¥ç€']

        for i in range(anchor, search_start - 1, -1):
            text = subtitles[i]['text']
            if any(starter in text for starter in scene_starters):
                return i
            if len(text) < 8 and '...' in text:
                return i + 1

        return search_start

    def _find_natural_end(self, subtitles: List[Dict], anchor: int, search_end: int) -> int:
        """å¯»æ‰¾è‡ªç„¶ç»“æŸç‚¹"""
        scene_enders = ['èµ°äº†', 'ç¦»å¼€', 'ç»“æŸ', 'ç®—äº†', 'å¥½å§', 'å†è§', 'æ‹œæ‹œ', 'å®Œäº†', 'è¡Œäº†']

        for i in range(anchor, search_end + 1):
            text = subtitles[i]['text']
            if any(ender in text for ender in scene_enders):
                return i
            if '...' in text and i > anchor + 8:
                return i

        return search_end

    def _extract_smart_content(self, subtitles: List[Dict], start_idx: int, end_idx: int) -> str:
        """æ™ºèƒ½æå–æ ¸å¿ƒå†…å®¹"""
        key_texts = []
        genre_keywords = self.get_genre_specific_keywords(self.detected_genre) if self.detected_genre else []

        for i in range(start_idx, min(end_idx + 1, start_idx + 6)):
            text = subtitles[i]['text']

            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®ä¿¡æ¯
            has_key_info = False
            if genre_keywords:
                has_key_info = any(keyword in text for keyword in genre_keywords)

            if has_key_info or any(keyword in text for keyword in self.universal_dramatic_keywords):
                key_texts.append(text.strip())

        return ' | '.join(key_texts) if key_texts else subtitles[start_idx]['text']

    def _extract_key_dialogue(self, subtitles: List[Dict], start_idx: int, end_idx: int) -> List[str]:
        """æå–å…³é”®å¯¹è¯"""
        key_dialogues = []

        for i in range(start_idx, end_idx + 1):
            text = subtitles[i]['text'].strip()
            score = self.calculate_intelligent_score(text)

            if score >= 4.0:
                time_code = f"{subtitles[i]['start']} --> {subtitles[i]['end']}"
                key_dialogues.append(f"[{time_code}] {text}")

        return key_dialogues[:5]

    def _analyze_smart_significance(self, subtitles: List[Dict], start_idx: int, end_idx: int) -> str:
        """æ™ºèƒ½åˆ†æå‰§æƒ…æ„ä¹‰"""
        content = ' '.join([subtitles[i]['text'] for i in range(start_idx, end_idx + 1)])
        significance_points = []

        # æ ¹æ®å‰§æƒ…ç±»å‹åˆ†æ
        if self.detected_genre == 'legal':
            if any(word in content for word in ['æ¡ˆä»¶', 'è¯æ®', 'å®¡åˆ¤']):
                significance_points.append("æ³•å¾‹ç¨‹åºæ¨è¿›")
        elif self.detected_genre == 'romance':
            if any(word in content for word in ['è¡¨ç™½', 'çº¦ä¼š', 'åˆ†æ‰‹']):
                significance_points.append("æƒ…æ„Ÿå…³ç³»å‘å±•")
        elif self.detected_genre == 'crime':
            if any(word in content for word in ['çº¿ç´¢', 'ç ´æ¡ˆ', 'çœŸç›¸']):
                significance_points.append("æ¡ˆä»¶è°ƒæŸ¥è¿›å±•")
        elif self.detected_genre == 'family':
            if any(word in content for word in ['å®¶åº­', 'äº²æƒ…', 'æˆé•¿']):
                significance_points.append("å®¶åº­å…³ç³»å‘å±•")

        # é€šç”¨æ„ä¹‰åˆ†æ
        if any(word in content for word in ['å‘ç°', 'çœŸç›¸', 'ç§˜å¯†']):
            significance_points.append("é‡è¦ä¿¡æ¯æ­éœ²")
        if any(word in content for word in ['å†³å®š', 'é€‰æ‹©', 'æ”¹å˜']):
            significance_points.append("è§’è‰²å‘å±•è½¬æŠ˜")
        if any(word in content for word in ['å†²çª', 'äº‰è®º', 'å¯¹æŠ—']):
            significance_points.append("æˆå‰§å†²çªé«˜æ½®")

        return "ã€".join(significance_points) if significance_points else "å‰§æƒ…é‡è¦èŠ‚ç‚¹"

    def _analyze_emotional_tone(self, subtitles: List[Dict], start_idx: int, end_idx: int) -> str:
        """åˆ†ææƒ…æ„ŸåŸºè°ƒ"""
        content = ' '.join([subtitles[i]['text'] for i in range(start_idx, end_idx + 1)])

        emotion_scores = {
            'positive': sum(1 for word in ['å¼€å¿ƒ', 'é«˜å…´', 'å¿«ä¹', 'å¹¸ç¦', 'æ¸©æš–', 'æ„ŸåŠ¨'] if word in content),
            'negative': sum(1 for word in ['æ‚²ä¼¤', 'éš¾è¿‡', 'ç—›è‹¦', 'ç»æœ›', 'æ„¤æ€’', 'ææƒ§'] if word in content),
            'tense': sum(1 for word in ['ç´§å¼ ', 'å±é™©', 'æ€¥', 'å¿«', 'å†²çª', 'äº‰è®º'] if word in content),
            'romantic': sum(1 for word in ['çˆ±', 'å–œæ¬¢', 'å¿ƒåŠ¨', 'æµªæ¼«', 'ç”œèœœ'] if word in content)
        }

        dominant_emotion = max(emotion_scores, key=emotion_scores.get)
        return dominant_emotion if emotion_scores[dominant_emotion] > 0 else 'neutral'

    def generate_smart_summary(self, episode_file: str, segments: List[Dict]) -> Dict:
        """ç”Ÿæˆæ™ºèƒ½å‰§é›†æ‘˜è¦"""
        episode_num = re.search(r'(\d+)', episode_file)
        episode_number = episode_num.group(1) if episode_num else "00"

        best_segments = segments[:2] if len(segments) >= 2 else segments
        total_duration = sum(seg['duration'] for seg in best_segments)

        # æ™ºèƒ½ç”Ÿæˆä¸»é¢˜
        if self.detected_genre:
            genre_desc = {
                'legal': 'æ³•å¾‹äº‰è®®',
                'romance': 'æƒ…æ„Ÿçº è‘›',
                'crime': 'æ¡ˆä»¶è°ƒæŸ¥',
                'family': 'å®¶åº­æ¸©æƒ…',
                'medical': 'åŒ»ç–—æ•‘æ²»',
                'business': 'å•†æˆ˜é£äº‘',
                'historical': 'å†å²å˜è¿',
                'fantasy': 'å¥‡å¹»å†’é™©'
            }.get(self.detected_genre, 'ç²¾å½©ç‰‡æ®µ')
        else:
            genre_desc = 'ç²¾å½©ç‰‡æ®µ'

        # ä»æ ¸å¿ƒå†…å®¹æå–å…³é”®ä¿¡æ¯
        key_elements = []
        for seg in best_segments:
            content = seg['core_content']
            if len(content) > 20:
                key_elements.append(content.split(' | ')[0][:15])

        theme_desc = 'ã€'.join(key_elements[:2]) if key_elements else genre_desc
        theme = f"E{episode_number}ï¼š{theme_desc}"

        return {
            'episode': episode_file,
            'episode_number': episode_number,
            'theme': theme,
            'genre': self.detected_genre or 'general',
            'genre_confidence': self.genre_confidence,
            'total_duration': total_duration,
            'segments': best_segments,
            'highlights': self._extract_smart_highlights(best_segments),
            'emotional_journey': self._analyze_emotional_journey(best_segments),
            'content_summary': self._generate_smart_content_summary(best_segments)
        }

    def _extract_smart_highlights(self, segments: List[Dict]) -> List[str]:
        """æ™ºèƒ½æå–äº®ç‚¹"""
        highlights = []

        for seg in segments:
            content_preview = seg['core_content'][:40] + "..." if len(seg['core_content']) > 40 else seg['core_content']

            # æ ¹æ®å‰§æƒ…ç±»å‹è°ƒæ•´äº®ç‚¹æè¿°
            if self.detected_genre == 'legal':
                value_desc = "æ³•å¾‹å†²çªæ ¸å¿ƒåœºé¢"
            elif self.detected_genre == 'romance':
                value_desc = "æƒ…æ„Ÿé«˜æ½®å…³é”®æ—¶åˆ»"
            elif self.detected_genre == 'crime':
                value_desc = "æ¡ˆä»¶ç ´è§£é‡è¦çº¿ç´¢"
            else:
                value_desc = "å‰§æƒ…å…³é”®è½¬æŠ˜ç‚¹"

            highlights.append(f"â€¢ {seg['significance']}ï¼š{content_preview} ({value_desc})")

        return highlights

    def _analyze_emotional_journey(self, segments: List[Dict]) -> str:
        """åˆ†ææƒ…æ„Ÿå†ç¨‹"""
        emotions = [seg.get('emotional_tone', 'neutral') for seg in segments]
        return " â†’ ".join(emotions)

    def _generate_smart_content_summary(self, segments: List[Dict]) -> str:
        """ç”Ÿæˆæ™ºèƒ½å†…å®¹æ‘˜è¦"""
        if not segments:
            return "æš‚æ— é‡ç‚¹å†…å®¹"

        significances = [seg['significance'] for seg in segments]
        return "ã€".join(set(significances))

    def time_to_seconds(self, time_str: str) -> float:
        """æ—¶é—´è½¬æ¢"""
        h, m, s_ms = time_str.split(':')
        s, ms = s_ms.split(',')
        return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000

    def analyze_single_episode(self, episode_file: str) -> Dict:
        """åˆ†æå•é›†"""
        print(f"ğŸ” æ™ºèƒ½åˆ†æ {episode_file}...")

        subtitles = self.parse_subtitle_file(episode_file)

        # é¦–æ¬¡åˆ†ææ—¶æ£€æµ‹å‰§æƒ…ç±»å‹
        if self.detected_genre is None:
            detected_genre = self.detect_genre(subtitles)
            print(f"ğŸ­ æ£€æµ‹åˆ°å‰§æƒ…ç±»å‹: {detected_genre} (ç½®ä¿¡åº¦: {self.genre_confidence:.2f})")

        core_segments = self.identify_smart_segments(subtitles)
        episode_summary = self.generate_smart_summary(episode_file, core_segments)

        # æ·»åŠ è¿è´¯æ€§åˆ†æ
        episode_summary['continuity_hints'] = self._analyze_episode_continuity(core_segments)
        episode_summary['next_episode_connection'] = self._generate_next_episode_hint(core_segments)

        return episode_summary

    def _analyze_episode_continuity(self, segments: List[Dict]) -> List[str]:
        """åˆ†æé›†å†…è¿è´¯æ€§æç¤º"""
        hints = []
        for i, segment in enumerate(segments):
            content = segment['core_content']

            # åˆ†ææƒ…èŠ‚æ¨è¿›çº¿ç´¢
            if 'æ¡ˆä»¶' in content or 'è¯æ®' in content:
                hints.append(f"ç‰‡æ®µ{i+1}: æ¡ˆä»¶è°ƒæŸ¥å…³é”®è¿›å±•")
            elif 'æ³•åº­' in content or 'å®¡åˆ¤' in content:
                hints.append(f"ç‰‡æ®µ{i+1}: æ³•åº­è¾©è®ºæ ¸å¿ƒåœºé¢")
            elif any(word in content for word in ['çœŸç›¸', 'å‘ç°', 'ç§˜å¯†']):
                hints.append(f"ç‰‡æ®µ{i+1}: é‡è¦ä¿¡æ¯æ­éœ²æ—¶åˆ»")
            elif any(word in content for word in ['å†³å®š', 'é€‰æ‹©', 'æ”¹å˜']):
                hints.append(f"ç‰‡æ®µ{i+1}: è§’è‰²å‘å±•è½¬æŠ˜ç‚¹")

        return hints

    def _generate_next_episode_hint(self, segments: List[Dict]) -> str:
        """ç”Ÿæˆä¸ä¸‹ä¸€é›†çš„è¡”æ¥æç¤º"""
        if not segments:
            return "æœ¬é›†ä¸ºç‹¬ç«‹æƒ…èŠ‚ï¼Œæ•¬è¯·æœŸå¾…ä¸‹é›†ç²¾å½©"

        last_segment = segments[-1]
        content = last_segment['core_content']

        # æ ¹æ®ç»“å°¾å†…å®¹é¢„æµ‹ä¸‹é›†æ–¹å‘
        if 'ç»§ç»­' in content or 'ä¸‹æ¬¡' in content:
            return "æœ¬é›†åŸ‹ä¸‹ä¼ç¬”ï¼Œä¸‹é›†å°†æœ‰é‡å¤§è¿›å±•"
        elif 'å¬è¯ä¼š' in content:
            return "å¬è¯ä¼šå‡†å¤‡å°±ç»ªï¼Œä¸‹é›†æ³•åº­æ¿€è¾©å³å°†å¼€å§‹"
        elif 'ç”³è¯‰' in content or 'é‡å®¡' in content:
            return "ç”³è¯‰ç¨‹åºå¯åŠ¨ï¼Œä¸‹é›†æ¡ˆä»¶è¿æ¥è½¬æœº"
        elif 'è¯æ®' in content:
            return "å…³é”®è¯æ®æµ®ç°ï¼Œä¸‹é›†çœŸç›¸å³å°†å¤§ç™½"
        elif 'å±é™©' in content or 'å¨èƒ' in content:
            return "å±æœºå‡çº§ï¼Œä¸‹é›†æƒ…å†µæ›´åŠ ç´§æ€¥"
        else:
            return "å‰§æƒ…æŒç»­å‘å±•ï¼Œä¸‹é›†æ›´å¤šç²¾å½©ç­‰æ‚¨è§‚çœ‹"

def analyze_all_episodes_intelligently():
    """æ™ºèƒ½åˆ†ææ•´ä¸ªå‰§é›†"""
    analyzer = IntelligentSubtitleAnalyzer()

    # è·å–æ‰€æœ‰å­—å¹•æ–‡ä»¶
    subtitle_files = [f for f in os.listdir('.') if f.endswith('.txt') and ('E' in f or 'e' in f)]
    subtitle_files.sort()

    if not subtitle_files:
        print("âŒ æœªæ‰¾åˆ°å­—å¹•æ–‡ä»¶")
        return []

    print(f"ğŸ“º æ™ºèƒ½åˆ†æç³»ç»Ÿå¯åŠ¨ - å‘ç° {len(subtitle_files)} ä¸ªå­—å¹•æ–‡ä»¶")
    print("ğŸ¤– ç³»ç»Ÿç‰¹æ€§ï¼š")
    print("â€¢ è‡ªåŠ¨è¯†åˆ«å‰§æƒ…ç±»å‹ï¼ˆæ³•å¾‹/çˆ±æƒ…/çŠ¯ç½ª/å®¶åº­/åŒ»ç–—/å•†æˆ˜/å¤è£…/å¥‡å¹»ï¼‰")
    print("â€¢ åŠ¨æ€è°ƒæ•´è¯„åˆ†æƒé‡å’Œæ—¶é•¿æ§åˆ¶")
    print("â€¢ AIè¾…åŠ©åˆ†ææå‡å‡†ç¡®æ€§")
    print("â€¢ æ™ºèƒ½é”™åˆ«å­—ä¿®æ­£å’Œç‰‡æ®µåˆå¹¶")
    print("=" * 60)

    all_episodes_plans = []

    for filename in subtitle_files:
        try:
            episode_plan = analyzer.analyze_single_episode(filename)
            all_episodes_plans.append(episode_plan)

            print(f"âœ“ {filename}")
            print(f"  ğŸ­ ç±»å‹: {episode_plan.get('genre', 'unknown')}")
            print(f"  ğŸ“ ä¸»é¢˜: {episode_plan['theme']}")
            print(f"  â±ï¸ æ—¶é•¿: {episode_plan['total_duration']:.1f}ç§’")
            print(f"  ğŸ¯ å†…å®¹: {episode_plan['content_summary']}")
            print(f"  ğŸ˜Š æƒ…æ„Ÿ: {episode_plan.get('emotional_journey', 'neutral')}")
            print()

        except Exception as e:
            print(f"âœ— é”™è¯¯å¤„ç† {filename}: {e}")

    # ç”Ÿæˆæ™ºèƒ½åˆ†ææŠ¥å‘Š
    generate_intelligent_report(all_episodes_plans, analyzer)

    return all_episodes_plans

def generate_intelligent_report(episodes_plans: List[Dict], analyzer):
    """ç”Ÿæˆæ™ºèƒ½åˆ†ææŠ¥å‘Š"""
    if not episodes_plans:
        return

    content = "ğŸ“º æ™ºèƒ½ç”µè§†å‰§å‰ªè¾‘åˆ†ææŠ¥å‘Š\n"
    content += "=" * 80 + "\n\n"

    # ç³»ç»Ÿä¿¡æ¯
    content += "ğŸ¤– æ™ºèƒ½åˆ†æç³»ç»Ÿä¿¡æ¯ï¼š\n"
    content += f"â€¢ æ£€æµ‹åˆ°çš„å‰§æƒ…ç±»å‹: {analyzer.detected_genre or 'é€šç”¨'}\n"
    content += f"â€¢ ç±»å‹è¯†åˆ«ç½®ä¿¡åº¦: {analyzer.genre_confidence:.2f}\n"
    content += f"â€¢ åˆ†ææ¨¡å¼: {'AIå¢å¼º' if analyzer.use_ai_analysis else 'è§„åˆ™åˆ†æ'}\n"
    content += f"â€¢ æ€»é›†æ•°: {len(episodes_plans)} é›†\n\n"

    # å‰§æƒ…ç±»å‹ç»Ÿè®¡
    genre_stats = {}
    for plan in episodes_plans:
        genre = plan.get('genre', 'unknown')
        genre_stats[genre] = genre_stats.get(genre, 0) + 1

    content += "ğŸ“Š å‰§æƒ…ç±»å‹åˆ†å¸ƒï¼š\n"
    for genre, count in genre_stats.items():
        content += f"â€¢ {genre}: {count} é›†\n"
    content += "\n"

    # è¯¦ç»†åˆ†æ
    total_duration = 0
    for i, plan in enumerate(episodes_plans):
        content += f"ğŸ“º {plan['theme']}\n"
        content += "-" * 60 + "\n"
        content += f"å‰§æƒ…ç±»å‹ï¼š{plan.get('genre', 'unknown')}\n"
        content += f"æ€»æ—¶é•¿ï¼š{plan['total_duration']:.1f} ç§’ ({plan['total_duration']/60:.1f} åˆ†é’Ÿ)\n"
        content += f"æƒ…æ„Ÿå†ç¨‹ï¼š{plan.get('emotional_journey', 'neutral')}\n"
        content += f"ç‰‡æ®µæ•°ï¼š{len(plan['segments'])} ä¸ªæ ¸å¿ƒç‰‡æ®µ\n\n"

        # ç‰‡æ®µè¯¦æƒ…
        for j, segment in enumerate(plan['segments']):
            content += f"ğŸ¬ ç‰‡æ®µ {j+1}ï¼š\n"
            content += f"   æ—¶é—´ï¼š{segment['start_time']} --> {segment['end_time']}\n"
            content += f"   æ—¶é•¿ï¼š{segment['duration']:.1f} ç§’\n"
            content += f"   æ„ä¹‰ï¼š{segment['significance']}\n"
            content += f"   æƒ…æ„Ÿï¼š{segment.get('emotional_tone', 'neutral')}\n"
            content += f"   ç½®ä¿¡åº¦ï¼š{segment.get('confidence', 0):.1f}\n"
            content += f"   å†…å®¹ï¼š{segment['core_content'][:100]}...\n"

            if segment['key_dialogue']:
                content += "   å…³é”®å¯¹è¯ï¼š\n"
                for dialogue in segment['key_dialogue'][:3]:
                    content += f"     {dialogue}\n"
            content += "\n"

        # äº®ç‚¹
        if plan['highlights']:
            content += "âœ¨ æœ¬é›†äº®ç‚¹ï¼š\n"
            for highlight in plan['highlights']:
                content += f"   {highlight}\n"

        total_duration += plan['total_duration']
        content += "=" * 80 + "\n\n"

    # æ€»ç»“
    content += f"ğŸ“Š æ™ºèƒ½åˆ†ææ€»ç»“ï¼š\n"
    content += f"â€¢ åˆ†æé›†æ•°ï¼š{len(episodes_plans)} é›†\n"
    content += f"â€¢ æ€»æ—¶é•¿ï¼š{total_duration:.1f} ç§’ ({total_duration/60:.1f} åˆ†é’Ÿ)\n"
    content += f"â€¢ å¹³å‡æ¯é›†ï¼š{total_duration/len(episodes_plans):.1f} ç§’\n"
    content += f"â€¢ ä¸»è¦ç±»å‹ï¼š{analyzer.detected_genre or 'é€šç”¨'}\n"
    content += f"â€¢ åˆ†æç²¾åº¦ï¼š{'AIå¢å¼ºæ¨¡å¼' if analyzer.use_ai_analysis else 'è§„åˆ™æ¨¡å¼'}\n"
    content += f"â€¢ é€‚ç”¨åœºæ™¯ï¼šè‡ªåŠ¨åŒ–å‰ªè¾‘ã€ç²¾å½©ç‰‡æ®µæå–ã€å†…å®¹åˆ†æ\n"

    # ä¿å­˜æŠ¥å‘Š
    safe_file_write('intelligent_analysis_report.txt', content)

    print(f"âœ… æ™ºèƒ½åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ° intelligent_analysis_report.txt")
    print(f"ğŸ“„ æŠ¥å‘ŠåŒ…å«{len(episodes_plans)}é›†çš„è¯¦ç»†åˆ†æï¼Œæ€»æ—¶é•¿{total_duration/60:.1f}åˆ†é’Ÿ")

if __name__ == "__main__":
    analyze_all_episodes_intelligently()
