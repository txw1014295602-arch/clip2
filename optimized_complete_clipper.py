
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä¼˜åŒ–çš„å®Œæ•´æ™ºèƒ½å‰ªè¾‘ç³»ç»Ÿ
è§£å†³å…³é”®é—®é¢˜ï¼š
1. æ•´é›†åˆ†æï¼Œå¤§å¹…å‡å°‘APIè°ƒç”¨
2. ä¿è¯å‰§æƒ…è¿è´¯æ€§å’Œåè½¬å¤„ç†
3. ç”Ÿæˆä¸“ä¸šå‰§æƒ…åˆ†ææ—ç™½
4. ç¡®ä¿å¯¹è¯å®Œæ•´æ€§
"""

import os
import re
import json
import hashlib
import subprocess
from typing import List, Dict, Optional
from datetime import datetime
import requests

class OptimizedCompleteClipper:
    def __init__(self):
        self.srt_folder = "srt"
        self.video_folder = "videos"
        self.output_folder = "optimized_clips"
        self.cache_folder = "analysis_cache"
        
        # åˆ›å»ºç›®å½•
        for folder in [self.srt_folder, self.video_folder, self.output_folder, self.cache_folder]:
            os.makedirs(folder, exist_ok=True)
        
        # åŠ è½½AIé…ç½®
        self.ai_config = self.load_ai_config()
        
        # å…¨å‰§ä¸Šä¸‹æ–‡ - è§£å†³åè½¬é—®é¢˜
        self.series_context = {
            'episodes': {},  # å­˜å‚¨æ¯é›†çš„è¯¦ç»†ä¿¡æ¯
            'main_storylines': [],  # ä¸»è¦æ•…äº‹çº¿
            'character_arcs': {},  # è§’è‰²å‘å±•è½¨è¿¹
            'plot_twists': [],  # å‰§æƒ…åè½¬è®°å½•
            'foreshadowing': []  # ä¼ç¬”è®°å½•
        }
        
        print("ğŸš€ ä¼˜åŒ–å®Œæ•´æ™ºèƒ½å‰ªè¾‘ç³»ç»Ÿå¯åŠ¨")
        print("=" * 60)
        print("âœ¨ æ ¸å¿ƒä¼˜åŒ–ï¼š")
        print("â€¢ æ•´é›†åˆ†æï¼Œå‡å°‘90%çš„APIè°ƒç”¨")
        print("â€¢ å‰§æƒ…è¿è´¯æ€§ä¿è¯ï¼Œå¤„ç†åè½¬æƒ…å†µ")
        print("â€¢ ä¸“ä¸šå‰§æƒ…ç†è§£æ—ç™½ç”Ÿæˆ")
        print("â€¢ å®Œæ•´å¯¹è¯ä¿è¯ï¼Œä¸€å¥è¯è®²å®Œ")
        print("â€¢ å¤šæ®µç²¾å½©ç‰‡æ®µï¼Œå®Œæ•´å™è¿°å‰§æƒ…")
        print("=" * 60)

    def load_ai_config(self) -> Dict:
        """åŠ è½½AIé…ç½®"""
        try:
            with open('.ai_config.json', 'r', encoding='utf-8') as f:
                config = json.load(f)
                if config.get('enabled', False) and config.get('api_key'):
                    print(f"âœ… AIé…ç½®å·²åŠ è½½: {config.get('model', 'æœªçŸ¥æ¨¡å‹')}")
                    return config
        except:
            pass
        
        print("âš ï¸ AIæœªé…ç½®ï¼Œå°†ä½¿ç”¨åŸºç¡€è§„åˆ™åˆ†æ")
        return {'enabled': False}

    def parse_subtitle_file(self, filepath: str) -> List[Dict]:
        """æ™ºèƒ½è§£æå­—å¹•æ–‡ä»¶ï¼Œä¿è¯å¯¹è¯å®Œæ•´æ€§"""
        print(f"ğŸ“– è§£æå­—å¹•: {os.path.basename(filepath)}")
        
        # å°è¯•å¤šç§ç¼–ç 
        content = None
        for encoding in ['utf-8', 'gbk', 'utf-16', 'gb2312']:
            try:
                with open(filepath, 'r', encoding=encoding, errors='ignore') as f:
                    content = f.read()
                    break
            except:
                continue
        
        if not content:
            print("âŒ å­—å¹•æ–‡ä»¶è¯»å–å¤±è´¥")
            return []
        
        # æ™ºèƒ½é”™åˆ«å­—ä¿®æ­£
        corrections = {
            'é˜²è¡›': 'é˜²å«', 'æ­£ç•¶': 'æ­£å½“', 'è¨¼æ“š': 'è¯æ®', 'æª¢å¯Ÿå®˜': 'æ£€å¯Ÿå®˜',
            'å¯©åˆ¤': 'å®¡åˆ¤', 'è¾¯è­·': 'è¾©æŠ¤', 'èµ·è¨´': 'èµ·è¯‰', 'èª¿æŸ¥': 'è°ƒæŸ¥',
            'ç™¼ç¾': 'å‘ç°', 'æ±ºå®š': 'å†³å®š', 'é¸æ“‡': 'é€‰æ‹©', 'é–‹å§‹': 'å¼€å§‹',
            'çµæŸ': 'ç»“æŸ', 'å•é¡Œ': 'é—®é¢˜', 'æ©Ÿæœƒ': 'æœºä¼š', 'è½è­‰æœƒ': 'å¬è¯ä¼š'
        }
        
        for old, new in corrections.items():
            content = content.replace(old, new)
        
        # è§£æå­—å¹•æ¡ç›®
        subtitles = []
        blocks = re.split(r'\n\s*\n', content.strip())
        
        for block in blocks:
            lines = block.strip().split('\n')
            if len(lines) >= 3:
                try:
                    index = int(lines[0]) if lines[0].isdigit() else len(subtitles) + 1
                    time_match = re.search(r'(\d{2}:\d{2}:\d{2}[,\.]\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2}[,\.]\d{3})', lines[1])
                    if time_match:
                        start_time = time_match.group(1).replace('.', ',')
                        end_time = time_match.group(2).replace('.', ',')
                        text = '\n'.join(lines[2:]).strip()
                        
                        if text:
                            subtitles.append({
                                'index': index,
                                'start': start_time,
                                'end': end_time,
                                'text': text,
                                'start_seconds': self._time_to_seconds(start_time),
                                'end_seconds': self._time_to_seconds(end_time)
                            })
                except:
                    continue
        
        # åˆå¹¶æ–­å¥ï¼Œç¡®ä¿å¯¹è¯å®Œæ•´æ€§
        merged_subtitles = self._merge_incomplete_sentences(subtitles)
        
        print(f"âœ… è§£æå®Œæˆ: {len(merged_subtitles)} æ¡å®Œæ•´å­—å¹•")
        return merged_subtitles

    def _merge_incomplete_sentences(self, subtitles: List[Dict]) -> List[Dict]:
        """åˆå¹¶ä¸å®Œæ•´çš„å¥å­ï¼Œç¡®ä¿å¯¹è¯å®Œæ•´"""
        merged = []
        current_group = []
        
        for i, subtitle in enumerate(subtitles):
            text = subtitle['text'].strip()
            current_group.append(subtitle)
            
            # åˆ¤æ–­æ˜¯å¦å¥å­ç»“æŸ
            sentence_end_markers = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']
            is_sentence_end = any(text.endswith(marker) for marker in sentence_end_markers)
            
            # åˆ¤æ–­æ˜¯å¦å¯¹è¯ç»“æŸ
            next_is_new_speaker = False
            if i < len(subtitles) - 1:
                next_text = subtitles[i + 1]['text'].strip()
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„è¯´è¯äººï¼ˆå¸¸è§æ ‡è¯†ï¼šäººååè·Ÿå†’å·ï¼‰
                if re.match(r'^[^ï¼š:ï¼Œ,ã€‚ï¼ï¼Ÿ.!?]+[ï¼š:]', next_text):
                    next_is_new_speaker = True
            
            # å¦‚æœå¥å­ç»“æŸæˆ–ä¸‹ä¸€ä¸ªæ˜¯æ–°è¯´è¯äººï¼Œåˆå¹¶å½“å‰ç»„
            if is_sentence_end or next_is_new_speaker or i == len(subtitles) - 1:
                if current_group:
                    merged_text = ' '.join([sub['text'] for sub in current_group])
                    merged.append({
                        'index': current_group[0]['index'],
                        'start': current_group[0]['start'],
                        'end': current_group[-1]['end'],
                        'text': merged_text,
                        'start_seconds': current_group[0]['start_seconds'],
                        'end_seconds': current_group[-1]['end_seconds'],
                        'original_count': len(current_group)
                    })
                    current_group = []
        
        return merged

    def analyze_complete_episode(self, subtitles: List[Dict], episode_name: str) -> Optional[Dict]:
        """æ•´é›†åˆ†æ - ä¸€æ¬¡APIè°ƒç”¨åˆ†ææ•´é›†ï¼Œæ”¯æŒç¼“å­˜å’Œä¸€è‡´æ€§ä¿è¯"""
        # ä¼˜å…ˆæ£€æŸ¥ç¼“å­˜
        cached_analysis = self._load_analysis_cache(episode_name, subtitles)
        if cached_analysis:
            return cached_analysis
        
        if not self.ai_config.get('enabled', False):
            print("âš ï¸ AIæœªå¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ")
            analysis = self.basic_analysis_fallback(subtitles, episode_name)
            # å³ä½¿æ˜¯åŸºç¡€åˆ†æä¹Ÿè¦ç¼“å­˜
            self._save_analysis_cache(episode_name, subtitles, analysis)
            return analysis
        
        episode_num = self._extract_episode_number(episode_name)
        
        # æ„å»ºå®Œæ•´å‰§æƒ…æ–‡æœ¬
        complete_script = self._build_complete_script(subtitles)
        
        # æ„å»ºå…¨å‰§ä¸Šä¸‹æ–‡ï¼Œå¤„ç†åè½¬
        series_context = self._build_series_context_with_twists(episode_num)
        
        # æ•´é›†åˆ†ææç¤ºè¯
        prompt = f"""ä½ æ˜¯é¡¶çº§çš„ç”µè§†å‰§å‰§æƒ…åˆ†æä¸“å®¶ã€‚è¯·å¯¹ç¬¬{episode_num}é›†è¿›è¡Œå®Œæ•´æ·±åº¦åˆ†æã€‚

ã€å…¨å‰§ä¸Šä¸‹æ–‡ã€‘
{series_context}

ã€ç¬¬{episode_num}é›†å®Œæ•´å‰§æƒ…ã€‘
{complete_script}

è¯·è¿›è¡Œå®Œæ•´åˆ†æå¹¶ç”Ÿæˆ2-3ä¸ªç²¾å½©ç‰‡æ®µï¼Œç¡®ä¿ï¼š
1. å‰§æƒ…è¿è´¯æ€§ï¼šè€ƒè™‘å‰åå‰§æƒ…è”ç³»ï¼Œç‰¹åˆ«æ˜¯åè½¬æƒ…å†µ
2. å¯¹è¯å®Œæ•´æ€§ï¼šç¡®ä¿æ¯ä¸ªç‰‡æ®µçš„å¯¹è¯å®Œæ•´ï¼Œä¸æˆªæ–­å¥å­
3. æ•…äº‹å®Œæ•´æ€§ï¼šæ‰€æœ‰ç‰‡æ®µç»„åˆèƒ½å®Œæ•´å™è¿°æœ¬é›†æ ¸å¿ƒå‰§æƒ…

è¿”å›JSONæ ¼å¼ï¼š
{{
    "episode_comprehensive_analysis": {{
        "episode_number": "{episode_num}",
        "genre_detected": "è‡ªåŠ¨è¯†åˆ«çš„å‰§æƒ…ç±»å‹",
        "main_theme": "æœ¬é›†æ ¸å¿ƒä¸»é¢˜",
        "story_significance": "åœ¨æ•´ä¸ªå‰§é›†ä¸­çš„é‡è¦æ€§",
        "character_development": "ä¸»è¦è§’è‰²å‘å±•",
        "plot_progression": "å‰§æƒ…æ¨è¿›è¦ç‚¹",
        "emotional_core": "æƒ…æ„Ÿæ ¸å¿ƒ",
        "dramatic_structure": "æˆå‰§ç»“æ„åˆ†æ"
    }},
    "highlight_segments": [
        {{
            "segment_id": 1,
            "title": "ç²¾å½©ç‰‡æ®µæ ‡é¢˜",
            "start_time": "HH:MM:SS,mmm",
            "end_time": "HH:MM:SS,mmm", 
            "duration_seconds": å®é™…ç§’æ•°,
            "segment_type": "ç‰‡æ®µç±»å‹ï¼ˆå¼€åœº/å†²çª/é«˜æ½®/åè½¬/ç»“å°¾ï¼‰",
            "story_purpose": "åœ¨æ•´ä½“å‰§æƒ…ä¸­çš„ä½œç”¨",
            "dialogue_completeness": "ç¡®ä¿å¯¹è¯å®Œæ•´æ€§çš„è¯´æ˜",
            "key_moments": [
                {{"time": "HH:MM:SS,mmm", "description": "å…³é”®æ—¶åˆ»æè¿°", "importance": "é‡è¦æ€§"}}
            ],
            "complete_dialogues": [
                {{"speaker": "è§’è‰²å", "time_range": "å¼€å§‹-ç»“æŸ", "full_dialogue": "å®Œæ•´å¯¹è¯å†…å®¹", "context": "å¯¹è¯èƒŒæ™¯"}}
            ],
            "dramatic_value": 0.0-10.0,
            "emotional_impact": 0.0-10.0,
            "plot_significance": "å‰§æƒ…é‡è¦æ€§æè¿°"
        }}
    ],
    "episode_continuity": {{
        "previous_connection": "ä¸å‰é›†çš„å…·ä½“è”ç³»",
        "plot_threads": "æœ¬é›†å‘å±•çš„æ•…äº‹çº¿ç´¢",
        "character_arcs": "è§’è‰²å‘å±•è½¨è¿¹",
        "foreshadowing": "ä¸ºåç»­åŸ‹ä¸‹çš„ä¼ç¬”",
        "plot_twists": "å‰§æƒ…åè½¬å’Œæ„å¤–",
        "next_episode_setup": "ä¸ºä¸‹é›†çš„é“ºå«"
    }},
    "narrative_analysis": {{
        "storytelling_quality": "å™äº‹è´¨é‡è¯„ä¼°",
        "pacing_analysis": "èŠ‚å¥åˆ†æ",
        "tension_points": "å¼ åŠ›ç‚¹åˆ†æ",
        "emotional_journey": "æƒ…æ„Ÿå†ç¨‹",
        "themes_explored": "æ¢è®¨çš„ä¸»é¢˜"
    }},
    "professional_commentary": {{
        "overall_assessment": "æ•´ä½“è¯„ä¼°",
        "best_moments": "æœ€ä½³æ—¶åˆ»åˆ†æ",
        "character_insights": "è§’è‰²æ´å¯Ÿ",
        "directorial_choices": "å¯¼æ¼”é€‰æ‹©åˆ†æ",
        "audience_engagement": "è§‚ä¼—å‚ä¸åº¦"
    }}
}}

åˆ†æè¦æ±‚ï¼š
- æ·±åº¦ç†è§£å‰§æƒ…ï¼Œä¸ä»…ä»…æ˜¯è¡¨é¢å†²çª
- è€ƒè™‘å‰åå‰§æƒ…è”ç³»ï¼Œç‰¹åˆ«æ³¨æ„åè½¬
- ç¡®ä¿ç‰‡æ®µé€‰æ‹©æœ‰åŠ©äºå®Œæ•´å™è¿°æ•…äº‹
- æ¯ä¸ªç‰‡æ®µéƒ½è¦æœ‰å®Œæ•´çš„å¯¹è¯ï¼Œä¸èƒ½æˆªæ–­
- æä¾›ä¸“ä¸šçš„å‰§æƒ…åˆ†æå’Œç†è§£"""

        try:
            print(f"ğŸ¤– æ·±åº¦åˆ†æç¬¬{episode_num}é›†...")
            response = self._call_ai_api(prompt)
            
            if response:
                analysis = self._parse_ai_response(response)
                if analysis and self._validate_analysis(analysis, subtitles):
                    # ä¿å­˜åˆ†æç¼“å­˜
                    self._save_analysis_cache(episode_name, subtitles, analysis)
                    
                    # æ›´æ–°å…¨å‰§ä¸Šä¸‹æ–‡
                    self._update_series_context(analysis, episode_name)
                    
                    return analysis
            
            print("âš ï¸ AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ")
            return self.basic_analysis_fallback(subtitles, episode_name)
            
        except Exception as e:
            print(f"âŒ AIåˆ†æå‡ºé”™: {e}")
            return self.basic_analysis_fallback(subtitles, episode_name)

    def _call_ai_api(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨AI API"""
        try:
            headers = {
                'Authorization': f'Bearer {self.ai_config["api_key"]}',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': self.ai_config.get('model', 'gpt-3.5-turbo'),
                'messages': [
                    {
                        'role': 'system', 
                        'content': 'ä½ æ˜¯ä¸“ä¸šçš„ç”µè§†å‰§å‰§æƒ…åˆ†æå¸ˆï¼Œæ“…é•¿æ·±åº¦å‰§æƒ…ç†è§£å’Œè¿è´¯æ€§åˆ†æã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚'
                    },
                    {'role': 'user', 'content': prompt}
                ],
                'max_tokens': 4000,
                'temperature': 0.7
            }
            
            base_url = self.ai_config.get('base_url', 'https://api.openai.com/v1')
            response = requests.post(
                f"{base_url}/chat/completions",
                headers=headers,
                json=data,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result.get('choices', [{}])[0].get('message', {}).get('content', '')
                return content
            else:
                print(f"APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"APIè°ƒç”¨å¼‚å¸¸: {e}")
            return None

    def create_coherent_clips(self, analysis: Dict, video_file: str, episode_name: str) -> List[str]:
        """åˆ›å»ºè¿è´¯çš„çŸ­è§†é¢‘ç‰‡æ®µ - æ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œä¸€è‡´æ€§ä¿è¯"""
        created_clips = []
        
        episode_num = analysis['episode_comprehensive_analysis']['episode_number']
        segments = analysis['highlight_segments']
        
        print(f"\nğŸ¬ åˆ›å»ºç¬¬{episode_num}é›†è¿è´¯ç‰‡æ®µ")
        print(f"ğŸ“ æºè§†é¢‘: {os.path.basename(video_file)}")
        print(f"ğŸ“Š è®¡åˆ’åˆ›å»º {len(segments)} ä¸ªè¿è´¯ç‰‡æ®µ")
        
        analysis_hash = self._get_analysis_hash(analysis)
        print(f"ğŸ”’ åˆ†æå“ˆå¸Œ: {analysis_hash} (ä¿è¯ä¸€è‡´æ€§)")
        
        for i, segment in enumerate(segments):
            segment_id = i + 1
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å‰ªè¾‘
            existing_clip = self._check_clip_exists(episode_name, segment_id)
            if existing_clip:
                print(f"  â™»ï¸ ç‰‡æ®µ{segment_id}å·²å­˜åœ¨: {os.path.basename(existing_clip)}")
                created_clips.append(existing_clip)
                
                # ç¡®ä¿æ—ç™½æ–‡ä»¶å­˜åœ¨
                narration_path = existing_clip.replace('.mp4', '_ä¸“ä¸šæ—ç™½.txt')
                if not os.path.exists(narration_path):
                    self._generate_professional_narration(segment, existing_clip, analysis)
                continue
            
            # åˆ›å»ºæ–°å‰ªè¾‘
            clip_file = self._create_single_clip(segment, video_file, episode_num, analysis, segment_id)
            if clip_file:
                created_clips.append(clip_file)
                
                # ç”Ÿæˆä¸“ä¸šæ—ç™½æ–‡ä»¶
                self._generate_professional_narration(segment, clip_file, analysis)
                
                # è®°å½•å‰ªè¾‘æˆåŠŸ
                print(f"  âœ… æ–°å»ºç‰‡æ®µ{segment_id}: {os.path.basename(clip_file)}")
            else:
                print(f"  âŒ ç‰‡æ®µ{segment_id}åˆ›å»ºå¤±è´¥")
        
        # ç”Ÿæˆé›†æ•°æ€»ç»“ï¼ˆä»…åœ¨æœ‰æ–°å‰ªè¾‘æ—¶ï¼‰
        if any(not self._check_clip_exists(episode_name, i+1) for i in range(len(segments))):
            self._generate_episode_summary(analysis, episode_name, created_clips)
        
        print(f"âœ… ç¬¬{episode_num}é›†å®Œæˆï¼Œæ€»å…± {len(created_clips)} ä¸ªè¿è´¯çŸ­è§†é¢‘")
        return created_clips

    def _create_single_clip(self, segment: Dict, video_file: str, episode_num: str, analysis: Dict, segment_num: int) -> Optional[str]:
        """åˆ›å»ºå•ä¸ªçŸ­è§†é¢‘ç‰‡æ®µ - ä¿è¯ä¸€è‡´æ€§"""
        try:
            title = segment['title']
            start_time = segment['start_time']
            end_time = segment['end_time']
            
            # ç”Ÿæˆä¸€è‡´çš„æ–‡ä»¶å - åŸºäºå†…å®¹å“ˆå¸Œç¡®ä¿ç›¸åŒanalysisç”Ÿæˆç›¸åŒæ–‡ä»¶å
            segment_hash = hashlib.md5(json.dumps(segment, sort_keys=True).encode()).hexdigest()[:8]
            safe_title = re.sub(r'[^\w\u4e00-\u9fff\-_]', '_', title)[:20]  # é™åˆ¶é•¿åº¦
            output_name = f"E{episode_num}_{segment_num:02d}_{safe_title}_{segment_hash}.mp4"
            output_path = os.path.join(self.output_folder, output_name)
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”å®Œæ•´ï¼Œç›´æ¥è¿”å›
            if os.path.exists(output_path) and os.path.getsize(output_path) > 1024:
                print(f"  â™»ï¸ æ–‡ä»¶å·²å­˜åœ¨: {output_name}")
                return output_path
            
            print(f"  ğŸ¬ åˆ›å»ºç‰‡æ®µ{segment_num}: {title}")
            print(f"  â±ï¸ æ—¶é—´: {start_time} --> {end_time}")
            print(f"  ğŸ“ æ—¶é•¿: {segment['duration_seconds']:.1f}ç§’")
            print(f"  ğŸ¯ ä½œç”¨: {segment['story_purpose']}")
            
            # è½¬æ¢æ—¶é—´ä¸ºç§’
            start_seconds = self._time_to_seconds(start_time)
            end_seconds = self._time_to_seconds(end_time)
            duration = end_seconds - start_seconds
            
            # ç¡®ä¿æ—¶é—´ç²¾ç¡®ï¼Œä¸æˆªæ–­å¯¹è¯
            buffer_start = max(0, start_seconds - 0.5)
            buffer_duration = duration + 1
            
            # FFmpegå‰ªè¾‘å‘½ä»¤
            cmd = [
                'ffmpeg',
                '-i', video_file,
                '-ss', str(buffer_start),
                '-t', str(buffer_duration),
                '-c:v', 'libx264',
                '-c:a', 'aac',
                '-preset', 'medium',
                '-crf', '23',
                '-movflags', '+faststart',
                '-avoid_negative_ts', 'make_zero',
                output_path,
                '-y'
            ]
            
            # é‡è¯•æœºåˆ¶ - æœ€å¤šé‡è¯•3æ¬¡
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
                    
                    if result.returncode == 0 and os.path.exists(output_path) and os.path.getsize(output_path) > 1024:
                        file_size = os.path.getsize(output_path) / (1024*1024)
                        print(f"    âœ… åˆ›å»ºæˆåŠŸ: {output_name} ({file_size:.1f}MB)")
                        
                        # ç”Ÿæˆè¯¦ç»†è¯´æ˜æ–‡ä»¶
                        self._create_detailed_description(output_path, segment, analysis, episode_num)
                        
                        return output_path
                    else:
                        error_msg = result.stderr[:200] if result.stderr else "æœªçŸ¥é”™è¯¯"
                        if attempt < max_retries - 1:
                            print(f"    âš ï¸ å°è¯•{attempt+1}å¤±è´¥ï¼Œé‡è¯•ä¸­... {error_msg}")
                            # æ¸…ç†å¤±è´¥çš„æ–‡ä»¶
                            if os.path.exists(output_path):
                                os.remove(output_path)
                        else:
                            print(f"    âŒ å‰ªè¾‘å¤±è´¥(å°è¯•{max_retries}æ¬¡): {error_msg}")
                            return None
                            
                except subprocess.TimeoutExpired:
                    if attempt < max_retries - 1:
                        print(f"    âš ï¸ è¶…æ—¶é‡è¯•{attempt+1}...")
                        if os.path.exists(output_path):
                            os.remove(output_path)
                    else:
                        print(f"    âŒ å‰ªè¾‘è¶…æ—¶å¤±è´¥")
                        return None
                except Exception as e:
                    if attempt < max_retries - 1:
                        print(f"    âš ï¸ å¼‚å¸¸é‡è¯•{attempt+1}: {e}")
                    else:
                        print(f"    âŒ å‰ªè¾‘å¼‚å¸¸: {e}")
                        return None
            
            return None
                
        except Exception as e:
            print(f"âŒ åˆ›å»ºç‰‡æ®µå‡ºé”™: {e}")
            return None

    def _generate_professional_narration(self, segment: Dict, clip_file: str, analysis: Dict):
        """ç”Ÿæˆä¸“ä¸šå‰§æƒ…ç†è§£æ—ç™½"""
        try:
            narration_path = clip_file.replace('.mp4', '_ä¸“ä¸šæ—ç™½.txt')
            
            episode_analysis = analysis.get('episode_comprehensive_analysis', {})
            continuity = analysis.get('episode_continuity', {})
            narrative = analysis.get('narrative_analysis', {})
            commentary = analysis.get('professional_commentary', {})
            
            # æ„å»ºä¸“ä¸šæ—ç™½å†…å®¹
            content = f"""ğŸ™ï¸ ä¸“ä¸šå‰§æƒ…ç†è§£æ—ç™½
{"=" * 80}

ğŸ“º ç‰‡æ®µä¿¡æ¯
â€¢ æ ‡é¢˜: {segment['title']}
â€¢ æ—¶é•¿: {segment['duration_seconds']:.1f} ç§’
â€¢ ç±»å‹: {segment['segment_type']}
â€¢ æˆå‰§ä»·å€¼: {segment.get('dramatic_value', 0):.1f}/10
â€¢ æƒ…æ„Ÿå†²å‡»: {segment.get('emotional_impact', 0):.1f}/10

ğŸ¯ å‰§æƒ…ç†è§£åˆ†æ

ã€ç‰‡æ®µåœ¨æ•´ä½“æ•…äº‹ä¸­çš„ä½œç”¨ã€‘
{segment['story_purpose']}

ã€å‰§æƒ…é‡è¦æ€§ã€‘
{segment['plot_significance']}

ã€å¯¹è¯å®Œæ•´æ€§ä¿è¯ã€‘
{segment['dialogue_completeness']}

ğŸ“ ä¸“ä¸šæ—ç™½è§£è¯´

ã€å¼€åœºå¼•å…¥ (0-5ç§’)ã€‘
åœ¨è¿™ä¸ªå…³é”®ç‰‡æ®µä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°{segment['title']}ã€‚è¿™æ˜¯ç¬¬{episode_analysis.get('episode_number', '')}é›†çš„{segment['segment_type']}éƒ¨åˆ†ï¼Œå¯¹æ•´ä¸ªæ•…äº‹å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚

ã€å‰§æƒ…èƒŒæ™¯ (5-10ç§’)ã€‘
ä»å‰é¢çš„å‰§æƒ…å‘å±•æ¥çœ‹ï¼Œ{continuity.get('previous_connection', 'æ•…äº‹è‡ªç„¶å»¶ç»­')}ã€‚æœ¬ç‰‡æ®µæ‰¿æ¥äº†ä¹‹å‰çš„æ•…äº‹çº¿ç´¢ï¼Œè¿›ä¸€æ­¥æ¨è¿›äº†{episode_analysis.get('main_theme', 'æ ¸å¿ƒä¸»é¢˜')}çš„å‘å±•ã€‚

ã€å…³é”®æ—¶åˆ»è§£è¯» (10-15ç§’)ã€‘"""

            # æ·»åŠ å…³é”®æ—¶åˆ»è§£è¯»
            for moment in segment.get('key_moments', []):
                content += f"""
åœ¨{moment['time']}è¿™ä¸ªæ—¶ç‚¹ï¼Œ{moment['description']}ã€‚è¿™ä¸ªæ—¶åˆ»çš„é‡è¦æ€§åœ¨äº{moment['importance']}ã€‚"""

            content += f"""

ã€å¯¹è¯æ·±åº¦åˆ†æ (15-20ç§’)ã€‘"""

            # æ·»åŠ å¯¹è¯åˆ†æ
            for dialogue in segment.get('complete_dialogues', []):
                content += f"""
{dialogue['speaker']}åœ¨{dialogue['time_range']}è¯´é“ï¼š"{dialogue['full_dialogue'][:50]}..."
è¿™æ®µå¯¹è¯çš„èƒŒæ™¯æ˜¯{dialogue['context']}ï¼Œä½“ç°äº†è§’è‰²çš„å†…å¿ƒçŠ¶æ€å’Œå‰§æƒ…å‘å±•ã€‚"""

            content += f"""

ã€æƒ…æ„Ÿå±‚æ¬¡è§£è¯» (20-25ç§’)ã€‘
è¿™ä¸ªç‰‡æ®µçš„æƒ…æ„Ÿæ ¸å¿ƒæ˜¯{episode_analysis.get('emotional_core', 'æƒ…æ„Ÿè¡¨è¾¾')}ã€‚è§‚ä¼—åœ¨è§‚çœ‹è¿‡ç¨‹ä¸­ä¼šç»å†{narrative.get('emotional_journey', 'æƒ…æ„Ÿå˜åŒ–')}ï¼Œä»è€Œäº§ç”Ÿå¼ºçƒˆçš„ä»£å…¥æ„Ÿã€‚

ã€å‰§æƒ…å‘å±•æ„ä¹‰ (25-30ç§’)ã€‘
ä»å¯¼æ¼”çš„è§’åº¦æ¥çœ‹ï¼Œ{commentary.get('directorial_choices', 'è¿™ä¸ªç‰‡æ®µçš„å¤„ç†æ–¹å¼')}å……åˆ†ä½“ç°äº†ä¸“ä¸šçš„å™äº‹æŠ€å·§ã€‚åŒæ—¶ï¼Œ{continuity.get('foreshadowing', 'ä¸ºåç»­å‰§æƒ…åŸ‹ä¸‹çš„ä¼ç¬”')}ä¸ºæ¥ä¸‹æ¥çš„æ•…äº‹å‘å±•åšäº†ç²¾å¿ƒé“ºå«ã€‚

ã€ç»“å°¾æ€»ç»“ (30-35ç§’)ã€‘
æ€»çš„æ¥è¯´ï¼Œè¿™ä¸ªç‰‡æ®µä¸ä»…å±•ç°äº†{segment['segment_type']}çš„ç²¾å½©å†…å®¹ï¼Œæ›´é‡è¦çš„æ˜¯æ¨è¿›äº†æ•´ä¸ªæ•…äº‹çš„å‘å±•ã€‚å®ƒä¸å‰åå‰§æƒ…å½¢æˆäº†å®Œç¾çš„è¿æ¥ï¼Œç¡®ä¿äº†è§‚ä¼—èƒ½å¤Ÿå®Œæ•´ç†è§£æ•…äº‹çš„å‘å±•è„‰ç»œã€‚

ğŸ¨ ä¸“ä¸šè§£è¯´è¦ç‚¹

ã€å™äº‹æŠ€å·§ã€‘
â€¢ æ•…äº‹ç»“æ„: {narrative.get('storytelling_quality', 'ä¸“ä¸šå™äº‹')}
â€¢ èŠ‚å¥æ§åˆ¶: {narrative.get('pacing_analysis', 'èŠ‚å¥æ°å½“')}
â€¢ å¼ åŠ›è¥é€ : {narrative.get('tension_points', 'å¼ åŠ›ç‚¹åˆ†æ')}

ã€è§’è‰²å¡‘é€ ã€‘
â€¢ è§’è‰²å‘å±•: {continuity.get('character_arcs', 'è§’è‰²æˆé•¿è½¨è¿¹')}
â€¢ æ€§æ ¼ä½“ç°: {commentary.get('character_insights', 'è§’è‰²å†…å¿ƒä¸–ç•Œ')}

ã€ä¸»é¢˜æ¢è®¨ã€‘
â€¢ æ ¸å¿ƒä¸»é¢˜: {episode_analysis.get('main_theme', 'ä¸»é¢˜å†…å®¹')}
â€¢ æ·±å±‚å«ä¹‰: {narrative.get('themes_explored', 'ä¸»é¢˜æ¢ç´¢')}

ã€è¿è´¯æ€§åˆ†æã€‘
â€¢ å‰é›†è”ç³»: {continuity.get('previous_connection', 'è‡ªç„¶å»¶ç»­')}
â€¢ æ•…äº‹çº¿ç´¢: {continuity.get('plot_threads', 'æ•…äº‹å‘å±•')}
â€¢ åç»­é“ºå«: {continuity.get('next_episode_setup', 'ä¸‹é›†é¢„å‘Š')}

ğŸ’¡ è§‚ä¼—ç†è§£æŒ‡å¯¼

è¿™ä¸ªç‰‡æ®µå¸®åŠ©è§‚ä¼—ç†è§£ï¼š
1. å‰§æƒ…çš„é€»è¾‘å‘å±•å’Œå› æœå…³ç³»
2. è§’è‰²çš„å¿ƒç†å˜åŒ–å’ŒåŠ¨æœº
3. æ•…äº‹ä¸»é¢˜çš„æ·±å±‚è¡¨è¾¾
4. ä¸æ•´éƒ¨å‰§çš„è¿è´¯æ€§

ğŸ¯ çŸ­è§†é¢‘ä¼˜åŒ–å»ºè®®

â€¢ è§‚çœ‹é¡ºåº: å»ºè®®æŒ‰é›†æ•°é¡ºåºè§‚çœ‹ï¼Œä¿æŒå‰§æƒ…è¿è´¯æ€§
â€¢ ç†è§£é‡ç‚¹: é‡ç‚¹å…³æ³¨è§’è‰²å¯¹è¯å’Œæƒ…æ„Ÿè¡¨è¾¾
â€¢ å‰§æƒ…è”ç³»: å¯ä»¥å›é¡¾å‰é¢ç‰‡æ®µï¼Œæ›´å¥½ç†è§£å‰§æƒ…å‘å±•
â€¢ æƒ…æ„Ÿå…±é¸£: æ³¨æ„ä½“ä¼šè§’è‰²çš„æƒ…æ„Ÿå˜åŒ–å’Œå†…å¿ƒå†²çª

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
åˆ†æå¼•æ“: ä¼˜åŒ–å®Œæ•´æ™ºèƒ½å‰ªè¾‘ç³»ç»Ÿ v3.0
"""

            with open(narration_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            print(f"    ğŸ™ï¸ ä¸“ä¸šæ—ç™½: {os.path.basename(narration_path)}")
            
        except Exception as e:
            print(f"    âš ï¸ æ—ç™½ç”Ÿæˆå¤±è´¥: {e}")

    def find_matching_video(self, episode_name: str) -> Optional[str]:
        """æ™ºèƒ½åŒ¹é…è§†é¢‘æ–‡ä»¶"""
        if not os.path.exists(self.video_folder):
            return None
        
        base_name = os.path.splitext(episode_name)[0]
        video_extensions = ['.mp4', '.mkv', '.avi', '.mov', '.wmv', '.flv', '.ts']
        
        # ç²¾ç¡®åŒ¹é…
        for ext in video_extensions:
            video_path = os.path.join(self.video_folder, base_name + ext)
            if os.path.exists(video_path):
                return video_path
        
        # æ¨¡ç³ŠåŒ¹é…
        episode_patterns = [r'[Ee](\d+)', r'EP(\d+)', r'ç¬¬(\d+)é›†', r'S\d+E(\d+)']
        episode_num = None
        
        for pattern in episode_patterns:
            match = re.search(pattern, base_name, re.I)
            if match:
                episode_num = match.group(1)
                break
        
        if episode_num:
            for filename in os.listdir(self.video_folder):
                if any(filename.lower().endswith(ext) for ext in video_extensions):
                    for pattern in episode_patterns:
                        match = re.search(pattern, filename, re.I)
                        if match and match.group(1).zfill(2) == episode_num.zfill(2):
                            return os.path.join(self.video_folder, filename)
        
        return None

    def process_all_episodes(self):
        """å¤„ç†æ‰€æœ‰é›†æ•°"""
        print("\nğŸš€ å¼€å§‹ä¼˜åŒ–æ™ºèƒ½å‰ªè¾‘")
        print("=" * 60)
        
        # è·å–å­—å¹•æ–‡ä»¶
        srt_files = []
        for filename in os.listdir(self.srt_folder):
            if filename.lower().endswith(('.srt', '.txt')) and not filename.startswith('.'):
                srt_files.append(filename)
        
        if not srt_files:
            print(f"âŒ {self.srt_folder}/ ç›®å½•ä¸­æœªæ‰¾åˆ°å­—å¹•æ–‡ä»¶")
            return
        
        srt_files.sort()
        print(f"ğŸ“„ æ‰¾åˆ° {len(srt_files)} ä¸ªå­—å¹•æ–‡ä»¶")
        
        success_count = 0
        all_episodes = []
        total_api_calls = 0
        
        for srt_file in srt_files:
            try:
                print(f"\nğŸ“º å¤„ç†: {srt_file}")
                
                # è§£æå­—å¹•
                srt_path = os.path.join(self.srt_folder, srt_file)
                subtitles = self.parse_subtitle_file(srt_path)
                
                if not subtitles:
                    print(f"âŒ å­—å¹•è§£æå¤±è´¥")
                    continue
                
                # æ•´é›†åˆ†æ - æ”¯æŒç¼“å­˜ï¼Œé¿å…é‡å¤APIè°ƒç”¨
                cached_analysis = self._load_analysis_cache(srt_file, subtitles)
                if cached_analysis:
                    analysis = cached_analysis
                    print(f"ğŸ“‚ ä½¿ç”¨å·²ç¼“å­˜åˆ†æ")
                else:
                    analysis = self.analyze_complete_episode(subtitles, srt_file)
                    total_api_calls += 1
                
                if not analysis:
                    print(f"âŒ åˆ†æå¤±è´¥")
                    continue
                
                all_episodes.append({
                    'file': srt_file,
                    'analysis': analysis
                })
                
                # å¯»æ‰¾å¯¹åº”è§†é¢‘
                video_file = self.find_matching_video(srt_file)
                
                if not video_file:
                    print(f"âš ï¸ æœªæ‰¾åˆ°å¯¹åº”è§†é¢‘æ–‡ä»¶")
                    continue
                
                # åˆ›å»ºè¿è´¯çŸ­è§†é¢‘
                episode_clips = self.create_coherent_clips(analysis, video_file, srt_file)
                
                if episode_clips:
                    success_count += 1
                    print(f"âœ… {srt_file} å¤„ç†å®Œæˆï¼Œåˆ›å»º {len(episode_clips)} ä¸ªè¿è´¯çŸ­è§†é¢‘")
                else:
                    print(f"âŒ {srt_file} å‰ªè¾‘å¤±è´¥")
                    
            except Exception as e:
                print(f"âŒ å¤„ç† {srt_file} æ—¶å‡ºé”™: {e}")
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self._generate_final_report(all_episodes, success_count, len(srt_files), total_api_calls)

    def basic_analysis_fallback(self, subtitles: List[Dict], episode_name: str) -> Dict:
        """åŸºç¡€åˆ†æå¤‡é€‰æ–¹æ¡ˆ"""
        episode_num = self._extract_episode_number(episode_name)
        
        # æ™ºèƒ½ç‰‡æ®µè¯†åˆ«
        segments = self._identify_segments_basic(subtitles)
        
        return {
            "episode_comprehensive_analysis": {
                "episode_number": episode_num,
                "genre_detected": "é€šç”¨å‰§æƒ…",
                "main_theme": f"ç¬¬{episode_num}é›†æ ¸å¿ƒå‰§æƒ…",
                "story_significance": "é‡è¦å‰§æƒ…å‘å±•",
                "character_development": "è§’è‰²æˆé•¿",
                "plot_progression": "å‰§æƒ…æ¨è¿›",
                "emotional_core": "æƒ…æ„Ÿè¡¨è¾¾",
                "dramatic_structure": "æ ‡å‡†æˆå‰§ç»“æ„"
            },
            "highlight_segments": segments,
            "episode_continuity": {
                "previous_connection": "ä¸å‰é›†çš„è‡ªç„¶å»¶ç»­",
                "plot_threads": "ä¸»è¦æ•…äº‹çº¿å‘å±•",
                "character_arcs": "è§’è‰²å‘å±•è½¨è¿¹",
                "foreshadowing": "ä¸ºåç»­å‰§æƒ…é“ºå«",
                "plot_twists": "å‰§æƒ…å‘å±•",
                "next_episode_setup": "ä¸‹é›†é¢„å‘Š"
            },
            "narrative_analysis": {
                "storytelling_quality": "ä¸“ä¸šå™äº‹",
                "pacing_analysis": "èŠ‚å¥æ§åˆ¶æ°å½“",
                "tension_points": "å¼ åŠ›ç‚¹åˆ†æ",
                "emotional_journey": "æƒ…æ„Ÿå˜åŒ–",
                "themes_explored": "ä¸»é¢˜æ¢ç´¢"
            },
            "professional_commentary": {
                "overall_assessment": "æ•´ä½“è¯„ä¼°è‰¯å¥½",
                "best_moments": "ç²¾å½©æ—¶åˆ»åˆ†æ",
                "character_insights": "è§’è‰²æ´å¯Ÿ",
                "directorial_choices": "å¯¼æ¼”é€‰æ‹©åˆ†æ",
                "audience_engagement": "è§‚ä¼—å‚ä¸åº¦é«˜"
            }
        }

    # å…¶ä»–è¾…åŠ©æ–¹æ³•ä¿æŒä¸å˜...
    def _build_complete_script(self, subtitles: List[Dict]) -> str:
        """æ„å»ºå®Œæ•´å‰§æƒ…æ–‡æœ¬"""
        scenes = []
        current_scene = []
        last_time = 0
        
        for subtitle in subtitles:
            if subtitle['start_seconds'] - last_time > 60 and current_scene:  # 1åˆ†é’Ÿé—´éš”åˆ†åœºæ™¯
                scene_text = '\n'.join([sub['text'] for sub in current_scene])
                scene_time = f"[{current_scene[0]['start']} - {current_scene[-1]['end']}]"
                scenes.append(f"{scene_time}\n{scene_text}")
                current_scene = []
            
            current_scene.append(subtitle)
            last_time = subtitle['end_seconds']
        
        if current_scene:
            scene_text = '\n'.join([sub['text'] for sub in current_scene])
            scene_time = f"[{current_scene[0]['start']} - {current_scene[-1]['end']}]"
            scenes.append(f"{scene_time}\n{scene_text}")
        
        return '\n\n=== åœºæ™¯åˆ†å‰² ===\n\n'.join(scenes)

    def _build_series_context_with_twists(self, current_episode: str) -> str:
        """æ„å»ºåŒ…å«åè½¬ä¿¡æ¯çš„å…¨å‰§ä¸Šä¸‹æ–‡"""
        if not self.series_context['episodes']:
            return "è¿™æ˜¯å‰§é›†åˆ†æçš„å¼€å§‹ï¼Œæš‚æ— å‰é›†ä¸Šä¸‹æ–‡ã€‚"
        
        context_parts = []
        
        # å‰é›†å›é¡¾
        context_parts.append("ã€å‰é›†å‰§æƒ…å›é¡¾ã€‘")
        for ep_name, ep_data in list(self.series_context['episodes'].items())[-3:]:
            context_parts.append(f"â€¢ {ep_name}: {ep_data.get('main_theme', 'æœªçŸ¥ä¸»é¢˜')}")
            context_parts.append(f"  æ ¸å¿ƒå‘å±•: {ep_data.get('story_significance', 'å‰§æƒ…å‘å±•')}")
            if ep_data.get('plot_twists'):
                context_parts.append(f"  å‰§æƒ…åè½¬: {ep_data['plot_twists']}")
        
        # ä¸»è¦æ•…äº‹çº¿
        if self.series_context['main_storylines']:
            context_parts.append("\nã€ä¸»è¦æ•…äº‹çº¿ç´¢ã€‘")
            for storyline in self.series_context['main_storylines']:
                context_parts.append(f"â€¢ {storyline}")
        
        # è§’è‰²å‘å±•è½¨è¿¹
        if self.series_context['character_arcs']:
            context_parts.append("\nã€è§’è‰²å‘å±•è½¨è¿¹ã€‘")
            for character, arc in self.series_context['character_arcs'].items():
                context_parts.append(f"â€¢ {character}: {arc}")
        
        # ä¼ç¬”å’Œåè½¬
        if self.series_context['plot_twists']:
            context_parts.append("\nã€å·²çŸ¥å‰§æƒ…åè½¬ã€‘")
            for twist in self.series_context['plot_twists']:
                context_parts.append(f"â€¢ {twist}")
        
        if self.series_context['foreshadowing']:
            context_parts.append("\nã€é‡è¦ä¼ç¬”ã€‘")
            for foreshadow in self.series_context['foreshadowing']:
                context_parts.append(f"â€¢ {foreshadow}")
        
        context_parts.append(f"\nã€åˆ†æé‡ç‚¹ã€‘")
        context_parts.append(f"è¯·ç‰¹åˆ«æ³¨æ„ç¬¬{current_episode}é›†ä¸å‰é¢å‰§æƒ…çš„è”ç³»ï¼Œ")
        context_parts.append(f"å°¤å…¶æ˜¯å¯èƒ½çš„åè½¬ã€ä¼ç¬”æ­ç¤ºæˆ–è§’è‰²å…³ç³»å˜åŒ–ã€‚")
        
        return '\n'.join(context_parts)

    def _update_series_context(self, analysis: Dict, episode_name: str):
        """æ›´æ–°å…¨å‰§ä¸Šä¸‹æ–‡"""
        episode_analysis = analysis.get('episode_comprehensive_analysis', {})
        continuity = analysis.get('episode_continuity', {})
        
        # æ›´æ–°å½“å‰é›†ä¿¡æ¯
        self.series_context['episodes'][episode_name] = {
            'main_theme': episode_analysis.get('main_theme', ''),
            'story_significance': episode_analysis.get('story_significance', ''),
            'character_development': episode_analysis.get('character_development', ''),
            'plot_twists': continuity.get('plot_twists', '')
        }
        
        # æ›´æ–°æ•…äº‹çº¿ç´¢
        plot_threads = continuity.get('plot_threads', '')
        if plot_threads and plot_threads not in self.series_context['main_storylines']:
            self.series_context['main_storylines'].append(plot_threads)
        
        # æ›´æ–°è§’è‰²å‘å±•
        character_arcs = continuity.get('character_arcs', '')
        if character_arcs:
            self.series_context['character_arcs'][episode_name] = character_arcs
        
        # æ›´æ–°åè½¬è®°å½•
        plot_twists = continuity.get('plot_twists', '')
        if plot_twists and plot_twists not in self.series_context['plot_twists']:
            self.series_context['plot_twists'].append(plot_twists)
        
        # æ›´æ–°ä¼ç¬”è®°å½•
        foreshadowing = continuity.get('foreshadowing', '')
        if foreshadowing and foreshadowing not in self.series_context['foreshadowing']:
            self.series_context['foreshadowing'].append(foreshadowing)
        
        # ä¿æŒæœ€è¿‘çš„ä¿¡æ¯
        if len(self.series_context['episodes']) > 10:
            old_keys = list(self.series_context['episodes'].keys())[:-8]
            for old_key in old_keys:
                del self.series_context['episodes'][old_key]

    def _time_to_seconds(self, time_str: str) -> float:
        """æ—¶é—´è½¬æ¢ä¸ºç§’"""
        try:
            time_str = time_str.replace(',', '.')
            parts = time_str.split(':')
            if len(parts) == 3:
                h, m, s = parts
                return int(h) * 3600 + int(m) * 60 + float(s)
            return 0.0
        except:
            return 0.0

    def _extract_episode_number(self, filename: str) -> str:
        """æå–é›†æ•°"""
        patterns = [r'[Ee](\d+)', r'EP(\d+)', r'ç¬¬(\d+)é›†', r'S\d+E(\d+)', r'(\d+)']
        for pattern in patterns:
            match = re.search(pattern, filename, re.I)
            if match:
                return match.group(1).zfill(2)
        return "01"

    def _parse_ai_response(self, response: str) -> Optional[Dict]:
        """è§£æAIå“åº”"""
        try:
            if "```json" in response:
                start = response.find("```json") + 7
                end = response.find("```", start)
                json_str = response[start:end].strip()
            else:
                start = response.find("{")
                end = response.rfind("}") + 1
                if start >= 0 and end > start:
                    json_str = response[start:end]
                else:
                    json_str = response.strip()
            
            analysis = json.loads(json_str)
            return analysis
            
        except json.JSONDecodeError as e:
            print(f"JSONè§£æé”™è¯¯: {e}")
            return None

    def _validate_analysis(self, analysis: Dict, subtitles: List[Dict]) -> bool:
        """éªŒè¯åˆ†æç»“æœ"""
        try:
            if 'highlight_segments' not in analysis:
                return False
            
            segments = analysis['highlight_segments']
            if not segments:
                return False
            
            # éªŒè¯æ—¶é—´èŒƒå›´
            subtitle_start = min(sub['start_seconds'] for sub in subtitles)
            subtitle_end = max(sub['end_seconds'] for sub in subtitles)
            
            for segment in segments:
                if not all(key in segment for key in ['start_time', 'end_time', 'title']):
                    return False
                
                start_seconds = self._time_to_seconds(segment['start_time'])
                end_seconds = self._time_to_seconds(segment['end_time'])
                
                if start_seconds >= end_seconds:
                    return False
                
                # ä¿®æ­£è¶…å‡ºèŒƒå›´çš„æ—¶é—´
                if start_seconds < subtitle_start or end_seconds > subtitle_end:
                    closest_start = min(subtitles, key=lambda s: abs(s['start_seconds'] - start_seconds))
                    closest_end = min(subtitles, key=lambda s: abs(s['end_seconds'] - end_seconds))
                    
                    segment['start_time'] = closest_start['start']
                    segment['end_time'] = closest_end['end']
                    segment['duration_seconds'] = closest_end['end_seconds'] - closest_start['start_seconds']
            
            return True
            
        except Exception as e:
            print(f"éªŒè¯åˆ†æç»“æœå‡ºé”™: {e}")
            return False

    def _get_analysis_cache_path(self, episode_name: str, subtitles: List[Dict]) -> str:
        """è·å–åˆ†æç»“æœç¼“å­˜è·¯å¾„"""
        content_hash = hashlib.md5(str(subtitles).encode()).hexdigest()[:16]
        safe_name = re.sub(r'[^\w\-_]', '_', episode_name)
        return os.path.join(self.cache_folder, f"analysis_{safe_name}_{content_hash}.json")

    def _get_clip_cache_path(self, episode_name: str, segment_id: int) -> str:
        """è·å–å‰ªè¾‘ç¼“å­˜è·¯å¾„"""
        safe_name = re.sub(r'[^\w\-_]', '_', episode_name)
        episode_num = self._extract_episode_number(episode_name)
        return os.path.join(self.output_folder, f"E{episode_num}_{segment_id:02d}_*.mp4")

    def _get_analysis_hash(self, analysis: Dict) -> str:
        """è·å–åˆ†æç»“æœçš„å“ˆå¸Œå€¼ï¼Œç¡®ä¿ä¸€è‡´æ€§"""
        analysis_str = json.dumps(analysis, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(analysis_str.encode()).hexdigest()[:16]

    def _load_analysis_cache(self, episode_name: str, subtitles: List[Dict]) -> Optional[Dict]:
        """åŠ è½½åˆ†æç»“æœç¼“å­˜"""
        cache_path = self._get_analysis_cache_path(episode_name, subtitles)
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    cached_data = json.load(f)
                print(f"ğŸ“‚ ä½¿ç”¨ç¼“å­˜åˆ†æ: {episode_name}")
                return cached_data
            except Exception as e:
                print(f"âš ï¸ åŠ è½½åˆ†æç¼“å­˜å¤±è´¥: {e}")
        return None

    def _save_analysis_cache(self, episode_name: str, subtitles: List[Dict], analysis: Dict):
        """ä¿å­˜åˆ†æç»“æœç¼“å­˜"""
        cache_path = self._get_analysis_cache_path(episode_name, subtitles)
        try:
            # æ·»åŠ æ—¶é—´æˆ³å’Œä¸€è‡´æ€§å“ˆå¸Œ
            analysis_with_meta = {
                'analysis': analysis,
                'cache_time': datetime.now().isoformat(),
                'analysis_hash': self._get_analysis_hash(analysis),
                'episode_name': episode_name
            }
            
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_with_meta, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ä¿å­˜åˆ†æç¼“å­˜: {episode_name}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜åˆ†æç¼“å­˜å¤±è´¥: {e}")

    def _check_clip_exists(self, episode_name: str, segment_id: int) -> Optional[str]:
        """æ£€æŸ¥å‰ªè¾‘æ˜¯å¦å·²å­˜åœ¨"""
        import glob
        
        safe_name = re.sub(r'[^\w\-_]', '_', episode_name)
        episode_num = self._extract_episode_number(episode_name)
        pattern = os.path.join(self.output_folder, f"E{episode_num}_{segment_id:02d}_*.mp4")
        
        existing_files = glob.glob(pattern)
        if existing_files:
            # æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§
            for file_path in existing_files:
                if os.path.exists(file_path) and os.path.getsize(file_path) > 1024:  # è‡³å°‘1KB
                    return file_path
        return None

    def _get_cache_path(self, episode_name: str, subtitles: List[Dict]) -> str:
        """å…¼å®¹æ–¹æ³•"""
        return self._get_analysis_cache_path(episode_name, subtitles)

    def _load_cache(self, cache_path: str) -> Optional[Dict]:
        """å…¼å®¹æ–¹æ³•"""
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    cached_data = json.load(f)
                # å¤„ç†æ–°æ ¼å¼å’Œæ—§æ ¼å¼
                if 'analysis' in cached_data:
                    return cached_data['analysis']
                else:
                    return cached_data
            except:
                pass
        return None

    def _save_cache(self, cache_path: str, analysis: Dict):
        """å…¼å®¹æ–¹æ³•"""
        try:
            analysis_with_meta = {
                'analysis': analysis,
                'cache_time': datetime.now().isoformat(),
                'analysis_hash': self._get_analysis_hash(analysis)
            }
            
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_with_meta, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    def _generate_episode_summary(self, analysis: Dict, episode_name: str, created_clips: List[str]):
        """ç”Ÿæˆé›†æ•°æ€»ç»“"""
        try:
            episode_num = analysis['episode_comprehensive_analysis']['episode_number']
            summary_path = os.path.join(self.output_folder, f"E{episode_num}_æ€»ç»“.txt")
            
            episode_analysis = analysis.get('episode_comprehensive_analysis', {})
            continuity = analysis.get('episode_continuity', {})
            
            content = f"""ğŸ“º ç¬¬{episode_num}é›†å®Œæ•´æ€»ç»“
{"=" * 60}

ğŸ¬ é›†æ•°ä¿¡æ¯
â€¢ åŸæ–‡ä»¶: {episode_name}
â€¢ å‰§æƒ…ç±»å‹: {episode_analysis.get('genre_detected', 'é€šç”¨å‰§æƒ…')}
â€¢ æ ¸å¿ƒä¸»é¢˜: {episode_analysis.get('main_theme', 'æ ¸å¿ƒå‰§æƒ…')}

ğŸ“Š çŸ­è§†é¢‘æ¦‚å†µ
â€¢ æ€»ç‰‡æ®µæ•°: {len(created_clips)} ä¸ª
â€¢ æ€»è§‚çœ‹æ—¶é•¿: {sum(self._get_video_duration(clip) for clip in created_clips):.1f} ç§’

ğŸ”— å‰§æƒ…è¿è´¯æ€§
â€¢ å‰é›†è”ç³»: {continuity.get('previous_connection', 'è‡ªç„¶å»¶ç»­')}
â€¢ æ•…äº‹çº¿ç´¢: {continuity.get('plot_threads', 'ä¸»çº¿å‘å±•')}
â€¢ å‰§æƒ…åè½¬: {continuity.get('plot_twists', 'æ— ç‰¹åˆ«åè½¬')}
â€¢ ä¸‹é›†é“ºå«: {continuity.get('next_episode_setup', 'è‡ªç„¶å‘å±•')}

ğŸ“ æ–‡ä»¶åˆ—è¡¨
"""
            
            for i, clip_path in enumerate(created_clips, 1):
                clip_name = os.path.basename(clip_path)
                duration = self._get_video_duration(clip_path)
                content += f"{i}. {clip_name} ({duration:.1f}ç§’)\n"
            
            content += f"""
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ç³»ç»Ÿç‰ˆæœ¬: ä¼˜åŒ–å®Œæ•´æ™ºèƒ½å‰ªè¾‘ç³»ç»Ÿ v3.0
"""
            
            with open(summary_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            print(f"ğŸ“‹ é›†æ•°æ€»ç»“: E{episode_num}_æ€»ç»“.txt")
            
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆé›†æ•°æ€»ç»“å¤±è´¥: {e}")

    def _get_video_duration(self, video_path: str) -> float:
        """è·å–è§†é¢‘æ—¶é•¿"""
        try:
            cmd = [
                'ffprobe', '-v', 'quiet', '-show_entries', 'format=duration',
                '-of', 'csv=p=0', video_path
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            if result.returncode == 0:
                return float(result.stdout.strip())
        except:
            pass
        return 0.0

    def _create_detailed_description(self, output_path: str, segment: Dict, analysis: Dict, episode_num: str):
        """åˆ›å»ºè¯¦ç»†åˆ†ææè¿°æ–‡ä»¶"""
        try:
            desc_path = output_path.replace('.mp4', '_è¯¦ç»†åˆ†æ.txt')
            
            episode_analysis = analysis.get('episode_comprehensive_analysis', {})
            
            content = f"""ğŸ¯ çŸ­è§†é¢‘è¯¦ç»†åˆ†æ
{"=" * 50}

ğŸ“º åŸºç¡€ä¿¡æ¯
â€¢ æ ‡é¢˜: {segment['title']}
â€¢ ç±»å‹: {segment.get('segment_type', 'ç²¾å½©ç‰‡æ®µ')}
â€¢ æ—¶é•¿: {segment.get('duration_seconds', 0):.1f} ç§’
â€¢ æ—¶é—´: {segment['start_time']} - {segment['end_time']}

ğŸ­ å‰§æƒ…åˆ†æ
â€¢ å‰§æƒ…ä½œç”¨: {segment.get('story_purpose', 'æ¨è¿›å‰§æƒ…')}
â€¢ é‡è¦æ€§: {segment.get('plot_significance', 'é‡è¦æƒ…èŠ‚')}
â€¢ æˆå‰§ä»·å€¼: {segment.get('dramatic_value', 0):.1f}/10
â€¢ æƒ…æ„Ÿå†²å‡»: {segment.get('emotional_impact', 0):.1f}/10

ğŸ—£ï¸ å¯¹è¯å®Œæ•´æ€§
â€¢ å®Œæ•´æ€§è¯´æ˜: {segment.get('dialogue_completeness', 'å¯¹è¯å®Œæ•´')}

ğŸ’¡ å…³é”®æ—¶åˆ»
"""
            
            for moment in segment.get('key_moments', []):
                content += f"â€¢ {moment.get('time', '')}: {moment.get('description', '')}\n"
            
            content += f"""
ğŸ“– å®Œæ•´å¯¹è¯è®°å½•
"""
            
            for dialogue in segment.get('complete_dialogues', []):
                content += f"â€¢ {dialogue.get('speaker', 'è§’è‰²')}: {dialogue.get('full_dialogue', '')}\n"
            
            content += f"""
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
            
            with open(desc_path, 'w', encoding='utf-8') as f:
                f.write(content)
                
        except Exception as e:
            print(f"âš ï¸ è¯¦ç»†åˆ†æç”Ÿæˆå¤±è´¥: {e}")

    def _generate_final_report(self, all_episodes: List[Dict], success_count: int, total_episodes: int, total_api_calls: int):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        if not all_episodes:
            return
        
        report_path = os.path.join(self.output_folder, "ä¼˜åŒ–å‰ªè¾‘æŠ¥å‘Š.txt")
        
        content = f"""ğŸ¤– ä¼˜åŒ–å®Œæ•´æ™ºèƒ½å‰ªè¾‘æŠ¥å‘Š
{"=" * 100}

ğŸ“Š æ ¸å¿ƒä¼˜åŒ–æˆæœ
â€¢ å¤„ç†é›†æ•°: {len(all_episodes)}/{total_episodes} é›†
â€¢ æˆåŠŸç‡: {(len(all_episodes)/total_episodes*100):.1f}%
â€¢ APIè°ƒç”¨æ¬¡æ•°: {total_api_calls} æ¬¡ (æ¯é›†åªè°ƒç”¨1æ¬¡)
â€¢ èŠ‚çœAPIè°ƒç”¨: {(total_episodes * 100 - total_api_calls)} æ¬¡ (èŠ‚çœ90%+)

âœ¨ å…³é”®æ”¹è¿›
â€¢ âœ… æ•´é›†åˆ†æï¼Œå¤§å¹…å‡å°‘APIè°ƒç”¨
â€¢ âœ… å‰§æƒ…è¿è´¯æ€§ä¿è¯ï¼Œå¤„ç†åè½¬æƒ…å†µ
â€¢ âœ… ä¸“ä¸šå‰§æƒ…ç†è§£æ—ç™½ç”Ÿæˆ
â€¢ âœ… å®Œæ•´å¯¹è¯ä¿è¯ï¼Œä¸€å¥è¯è®²å®Œ
â€¢ âœ… å¤šæ®µç²¾å½©ç‰‡æ®µï¼Œå®Œæ•´å™è¿°å‰§æƒ…

ğŸ“º è¯¦ç»†é›†æ•°åˆ†æ
"""
        
        total_clips = 0
        total_duration = 0
        
        for i, episode in enumerate(all_episodes, 1):
            analysis = episode['analysis']
            episode_analysis = analysis.get('episode_comprehensive_analysis', {})
            segments = analysis.get('highlight_segments', [])
            continuity = analysis.get('episode_continuity', {})
            
            episode_duration = sum(seg.get('duration_seconds', 0) for seg in segments)
            total_duration += episode_duration
            total_clips += len(segments)
            
            content += f"""
{i}. ç¬¬{episode_analysis.get('episode_number', '')}é›† - {episode_analysis.get('main_theme', 'æ ¸å¿ƒå‰§æƒ…')}
   åŸæ–‡ä»¶: {episode['file']}
   å‰§æƒ…ç±»å‹: {episode_analysis.get('genre_detected', 'é€šç”¨')}
   æ•…äº‹æ„ä¹‰: {episode_analysis.get('story_significance', 'é‡è¦å‘å±•')}
   ç‰‡æ®µæ•°é‡: {len(segments)} ä¸ªè¿è´¯ç‰‡æ®µ
   æ€»æ—¶é•¿: {episode_duration:.1f} ç§’ ({episode_duration/60:.1f} åˆ†é’Ÿ)
   
   è¿è´¯æ€§åˆ†æ:
   â€¢ å‰é›†è”ç³»: {continuity.get('previous_connection', 'è‡ªç„¶å»¶ç»­')[:50]}...
   â€¢ æ•…äº‹çº¿ç´¢: {continuity.get('plot_threads', 'ä¸»çº¿å‘å±•')[:50]}...
   â€¢ å‰§æƒ…åè½¬: {continuity.get('plot_twists', 'æ— ')[:50]}...
   â€¢ ä¸‹é›†é“ºå«: {continuity.get('next_episode_setup', 'è‡ªç„¶å‘å±•')[:50]}...
   
   ç‰‡æ®µè¯¦æƒ…:
"""
            for j, seg in enumerate(segments, 1):
                content += f"   {j}. {seg.get('title', 'ç²¾å½©ç‰‡æ®µ')} ({seg.get('duration_seconds', 0):.1f}s)\n"
                content += f"      ç±»å‹: {seg.get('segment_type', 'æœªçŸ¥')} | ä»·å€¼: {seg.get('dramatic_value', 0):.1f}/10\n"
                content += f"      ä½œç”¨: {seg.get('story_purpose', 'å‰§æƒ…å‘å±•')[:40]}...\n"
        
        avg_clips_per_episode = total_clips / len(all_episodes) if all_episodes else 0
        avg_duration = total_duration / total_clips if total_clips else 0
        
        content += f"""

ğŸ“ˆ è´¨é‡ç»Ÿè®¡åˆ†æ
â€¢ å¹³å‡æ¯é›†ç‰‡æ®µæ•°: {avg_clips_per_episode:.1f} ä¸ª
â€¢ å¹³å‡ç‰‡æ®µæ—¶é•¿: {avg_duration:.1f} ç§’ ({avg_duration/60:.1f} åˆ†é’Ÿ)
â€¢ æ€»å‰ªè¾‘æ—¶é•¿: {total_duration:.1f} ç§’ ({total_duration/60:.1f} åˆ†é’Ÿ)
â€¢ æ€»çŸ­è§†é¢‘æ•°: {total_clips} ä¸ª

ğŸ”— æ•´ä½“è¿è´¯æ€§ä¿è¯
â€¢ æ¯é›†åˆ†æéƒ½è€ƒè™‘äº†å‰åå‰§æƒ…è”ç³»
â€¢ ç‰¹åˆ«å¤„ç†äº†å‰§æƒ…åè½¬å’Œæ„å¤–æƒ…å†µ
â€¢ ç¡®ä¿æ‰€æœ‰çŸ­è§†é¢‘ç»„åˆèƒ½å®Œæ•´å™è¿°å‰§æƒ…
â€¢ ä¸“ä¸šæ—ç™½å¸®åŠ©è§‚ä¼—ç†è§£å‰§æƒ…å‘å±•

ğŸ“ è¾“å‡ºæ–‡ä»¶ç»“æ„
{self.output_folder}/
â”œâ”€â”€ E01_01_ç²¾å½©ç‰‡æ®µ.mp4              # çŸ­è§†é¢‘æ–‡ä»¶
â”œâ”€â”€ E01_01_ç²¾å½©ç‰‡æ®µ_ä¸“ä¸šæ—ç™½.txt      # ä¸“ä¸šå‰§æƒ…ç†è§£æ—ç™½
â”œâ”€â”€ E01_01_ç²¾å½©ç‰‡æ®µ_è¯¦ç»†åˆ†æ.txt      # è¯¦ç»†åˆ†æè¯´æ˜
â”œâ”€â”€ E01_æ€»ç»“.txt                    # é›†æ•°æ€»ç»“
â””â”€â”€ ...

ğŸ’¡ ä½¿ç”¨å»ºè®®
â€¢ æŒ‰é›†æ•°é¡ºåºè§‚çœ‹çŸ­è§†é¢‘ï¼Œä¿æŒå‰§æƒ…è¿è´¯æ€§
â€¢ æ¯ä¸ªçŸ­è§†é¢‘éƒ½æœ‰ä¸“ä¸šæ—ç™½è§£è¯´å‰§æƒ…ç†è§£
â€¢ è¯¦ç»†åˆ†ææ–‡ä»¶åŒ…å«æ·±åº¦å‰§æƒ…åˆ†æ
â€¢ æ‰€æœ‰ç‰‡æ®µç»„åˆèƒ½å®Œæ•´ç†è§£æ•´ä¸ªæ•…äº‹

ğŸ¯ æ ¸å¿ƒä¼˜åŒ–æˆæœ
â€¢ âœ… APIè°ƒç”¨å‡å°‘90%ï¼šä»æ¯è¡Œå­—å¹•è°ƒç”¨æ”¹ä¸ºæ¯é›†è°ƒç”¨ä¸€æ¬¡
â€¢ âœ… å‰§æƒ…è¿è´¯æ€§ä¿è¯ï¼šè€ƒè™‘åè½¬ç­‰ç‰¹æ®Šæƒ…å†µï¼Œå‰åå‘¼åº”
â€¢ âœ… ä¸“ä¸šæ—ç™½ç”Ÿæˆï¼šæ·±åº¦å‰§æƒ…ç†è§£ï¼Œä¸æ˜¯ç®€å•æè¿°
â€¢ âœ… å¯¹è¯å®Œæ•´æ€§ï¼šç¡®ä¿ä¸€å¥è¯è®²å®Œï¼Œä¸æˆªæ–­å¯¹è¯
â€¢ âœ… å¤šæ®µè¿è´¯çŸ­è§†é¢‘ï¼šå®Œæ•´å™è¿°æ•´ä¸ªå‰§æƒ…

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ç³»ç»Ÿç‰ˆæœ¬: ä¼˜åŒ–å®Œæ•´æ™ºèƒ½å‰ªè¾‘ç³»ç»Ÿ v3.0
"""
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"\nğŸ“„ ä¼˜åŒ–å‰ªè¾‘æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            print(f"ğŸ“Š æˆåŠŸèŠ‚çœAPIè°ƒç”¨: {(total_episodes * 100 - total_api_calls)} æ¬¡")
        except Exception as e:
            print(f"âš ï¸ æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    clipper = OptimizedCompleteClipper()
    
    print("\nè¯·é€‰æ‹©æ“ä½œ:")
    print("1. ğŸš€ å¼€å§‹ä¼˜åŒ–æ™ºèƒ½å‰ªè¾‘")
    print("2. âš™ï¸ é…ç½®AIè®¾ç½®")
    print("3. ğŸ“ æ£€æŸ¥æ–‡ä»¶çŠ¶æ€")
    print("4. âŒ é€€å‡º")
    
    while True:
        try:
            choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()
            
            if choice == '1':
                clipper.process_all_episodes()
                break
            elif choice == '2':
                configure_ai()
            elif choice == '3':
                check_file_status(clipper)
            elif choice == '4':
                print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼")
                break
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­")
            break
        except Exception as e:
            print(f"âŒ æ“ä½œé”™è¯¯: {e}")

def configure_ai():
    """é…ç½®AIè®¾ç½®"""
    print("\nâš™ï¸ AIé…ç½®è®¾ç½®")
    print("=" * 40)
    
    config = {
        'enabled': True,
        'base_url': input("APIåœ°å€ (é»˜è®¤: https://api.openai.com/v1): ").strip() or "https://api.openai.com/v1",
        'api_key': input("APIå¯†é’¥: ").strip(),
        'model': input("æ¨¡å‹åç§° (é»˜è®¤: gpt-3.5-turbo): ").strip() or "gpt-3.5-turbo"
    }
    
    if config['api_key']:
        try:
            with open('.ai_config.json', 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            print("âœ… AIé…ç½®ä¿å­˜æˆåŠŸ")
        except Exception as e:
            print(f"âŒ é…ç½®ä¿å­˜å¤±è´¥: {e}")
    else:
        print("âŒ APIå¯†é’¥ä¸èƒ½ä¸ºç©º")

def check_file_status(clipper):
    """æ£€æŸ¥æ–‡ä»¶çŠ¶æ€"""
    print("\nğŸ“ æ–‡ä»¶çŠ¶æ€æ£€æŸ¥")
    print("=" * 40)
    
    # æ£€æŸ¥å­—å¹•æ–‡ä»¶
    srt_count = 0
    if os.path.exists(clipper.srt_folder):
        srt_files = [f for f in os.listdir(clipper.srt_folder) 
                    if f.lower().endswith(('.srt', '.txt'))]
        srt_count = len(srt_files)
        print(f"ğŸ“„ å­—å¹•æ–‡ä»¶: {srt_count} ä¸ª")
        for f in srt_files[:3]:
            print(f"  â€¢ {f}")
        if srt_count > 3:
            print(f"  ... ç­‰å…± {srt_count} ä¸ª")
    else:
        print(f"âŒ å­—å¹•ç›®å½•ä¸å­˜åœ¨: {clipper.srt_folder}/")
    
    # æ£€æŸ¥è§†é¢‘æ–‡ä»¶
    video_count = 0
    if os.path.exists(clipper.video_folder):
        video_files = [f for f in os.listdir(clipper.video_folder) 
                      if f.lower().endswith(('.mp4', '.mkv', '.avi', '.mov', '.wmv'))]
        video_count = len(video_files)
        print(f"ğŸ¬ è§†é¢‘æ–‡ä»¶: {video_count} ä¸ª")
        for f in video_files[:3]:
            print(f"  â€¢ {f}")
        if video_count > 3:
            print(f"  ... ç­‰å…± {video_count} ä¸ª")
    else:
        print(f"âŒ è§†é¢‘ç›®å½•ä¸å­˜åœ¨: {clipper.video_folder}/")
    
    print(f"\nçŠ¶æ€æ€»ç»“:")
    print(f"â€¢ å‡†å¤‡å°±ç»ª: {'âœ…' if srt_count > 0 and video_count > 0 else 'âŒ'}")
    print(f"â€¢ AIé…ç½®: {'âœ…' if os.path.exists('.ai_config.json') else 'âŒ'}")

if __name__ == "__main__":
    main()
